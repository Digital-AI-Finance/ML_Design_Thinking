% Pandoc LaTeX template for course handouts
% Usage: pandoc input.md -o output.tex --template=handout_template.tex --listings
\documentclass[10pt,a4paper]{article}

% Geometry
\usepackage[margin=2.5cm]{geometry}

% Fonts and encoding
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}

% Math
\usepackage{amsmath,amssymb,amsfonts}

% Tables
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}

% Graphics
\usepackage{graphicx}

% Colors
\usepackage{xcolor}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{codebg}{RGB}{248,248,248}
\definecolor{codeframe}{RGB}{200,200,200}

% Code listings
\usepackage{listings}
\lstset{
  backgroundcolor=\color{codebg},
  basicstyle=\ttfamily\small,
  breakatwhitespace=false,
  breaklines=true,
  captionpos=b,
  commentstyle=\color{gray},
  frame=single,
  framerule=0.5pt,
  rulecolor=\color{codeframe},
  keepspaces=true,
  keywordstyle=\color{mlblue}\bfseries,
  language=Python,
  numbers=none,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  stringstyle=\color{mlpurple},
  tabsize=4,
  xleftmargin=0.5em,
  xrightmargin=0.5em,
  aboveskip=1em,
  belowskip=1em
}

% Hyperlinks
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=mlblue,
  urlcolor=mlblue,
  citecolor=mlblue
}

% Headers and footers
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\lhead{\textcolor{mlpurple}{\textbf{Topic Modeling}}}
\rhead{\textcolor{gray}{Intermediate}}
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt}

% Paragraph spacing
\setlength{\parindent}{0pt}
\setlength{\parskip}{0.5em}

% Section formatting
\usepackage{titlesec}
\titleformat{\section}
  {\Large\bfseries\color{mlpurple}}
  {\thesection}{1em}{}
\titleformat{\subsection}
  {\large\bfseries\color{mlblue}}
  {\thesubsection}{1em}{}
\titleformat{\subsubsection}
  {\normalsize\bfseries}
  {\thesubsubsection}{1em}{}

% Lists
\usepackage{enumitem}
\setlist[itemize]{topsep=0.5em, itemsep=0.25em}
\setlist[enumerate]{topsep=0.5em, itemsep=0.25em}

% Tight list support for pandoc
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% Title
\title{\Huge\bfseries\color{mlpurple} Handout 2: Implementing LDA for Topic Discovery (Intermediate Level)}
\author{Machine Learning for Smarter Innovation}
\date{}

\begin{document}

\maketitle
\thispagestyle{fancy}

\section{Handout 2: Implementing LDA for Topic Discovery (Intermediate
Level)}\label{handout-2-implementing-lda-for-topic-discovery-intermediate-level}

\subsection{Understanding Latent Dirichlet Allocation
(LDA)}\label{understanding-latent-dirichlet-allocation-lda}

LDA is a probabilistic model that discovers topics by assuming documents
are mixtures of topics, and topics are mixtures of words. It's the
industry standard for topic modeling.

\subsection{How LDA Works}\label{how-lda-works}

\subsubsection{The Generative Story}\label{the-generative-story}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{For each topic}: Define a distribution over words
\item
  \textbf{For each document}:

  \begin{itemize}
  \tightlist
  \item
    Choose a distribution over topics
  \item
    For each word position:

    \begin{itemize}
    \tightlist
    \item
      Pick a topic from the document's distribution
    \item
      Pick a word from that topic's distribution
    \end{itemize}
  \end{itemize}
\end{enumerate}

\subsubsection{Key Parameters}\label{key-parameters}

\begin{itemize}
\tightlist
\item
  \textbf{num\_topics (K)}: How many topics to find
\item
  \textbf{alpha}: Document-topic density (lower = fewer topics per doc)
\item
  \textbf{beta/eta}: Topic-word density (lower = fewer words per topic)
\end{itemize}

\subsection{Complete Implementation
Guide}\label{complete-implementation-guide}

\subsubsection{Step 1: Data Preparation}\label{step-1-data-preparation}

\begin{lstlisting}[language=Python]
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Download required NLTK data
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

# Initialize tools
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    """Clean and prepare text for topic modeling."""
    # Lowercase
    text = text.lower()

    # Remove special characters, keep only letters and spaces
    text = re.sub(r'[^a-z\s]', '', text)

    # Tokenize
    tokens = text.split()

    # Remove stopwords and short words
    tokens = [token for token in tokens
              if token not in stop_words and len(token) > 2]

    # Lemmatize
    tokens = [lemmatizer.lemmatize(token) for token in tokens]

    return tokens

# Load your data
df = pd.DataFrame({
    'text': [
        "The new smartphone has amazing battery life and fast charging",
        "I love the design, it's sleek and modern looking",
        "Customer service was helpful when I had issues",
        # ... more documents
    ]
})

# Preprocess all documents
processed_docs = df['text'].apply(preprocess_text).tolist()
\end{lstlisting}

\subsubsection{Step 2: Build LDA Model}\label{step-2-build-lda-model}

\begin{lstlisting}[language=Python]
from gensim import corpora, models
import numpy as np

# Create dictionary and corpus
dictionary = corpora.Dictionary(processed_docs)

# Filter extremes (optional but recommended)
dictionary.filter_extremes(
    no_below=2,      # Ignore words in less than 2 documents
    no_above=0.5,    # Ignore words in more than 50% of documents
    keep_n=1000      # Keep top 1000 most frequent words
)

# Create bag-of-words representation
corpus = [dictionary.doc2bow(doc) for doc in processed_docs]

# Build LDA model
lda_model = models.LdaModel(
    corpus=corpus,
    id2word=dictionary,
    num_topics=5,              # Number of topics
    random_state=42,           # For reproducibility
    passes=10,                 # Number of passes through corpus
    alpha='auto',              # Learn optimal alpha
    per_word_topics=True       # Compute word-topic probabilities
)
\end{lstlisting}

\subsubsection{Step 3: Explore Topics}\label{step-3-explore-topics}

\begin{lstlisting}[language=Python]
# Print topics with top words
def display_topics(model, num_words=10):
    """Display topics with their top words."""
    for idx, topic in model.print_topics(num_words=num_words):
        print(f"\nTopic {idx}:")
        # Parse the topic string
        words = topic.split('+')
        for word in words:
            prob, term = word.split('*')
            term = term.strip().strip('"')
            print(f"  {term}: {float(prob):.3f}")

display_topics(lda_model)

# Get topic distribution for a specific document
doc_topics = lda_model.get_document_topics(corpus[0])
print(f"\nDocument 0 topic distribution:")
for topic_id, prob in doc_topics:
    print(f"  Topic {topic_id}: {prob:.3f}")
\end{lstlisting}

\subsubsection{Step 4: Evaluate Model
Quality}\label{step-4-evaluate-model-quality}

\begin{lstlisting}[language=Python]
from gensim.models import CoherenceModel

# Calculate coherence score
coherence_model = CoherenceModel(
    model=lda_model,
    texts=processed_docs,
    dictionary=dictionary,
    coherence='c_v'
)

coherence_score = coherence_model.get_coherence()
print(f"\nCoherence Score: {coherence_score:.3f}")

# Interpretation:
# > 0.5: Good
# 0.4-0.5: Acceptable
# < 0.4: Poor (try different parameters)
\end{lstlisting}

\subsubsection{Step 5: Optimize Number of
Topics}\label{step-5-optimize-number-of-topics}

\begin{lstlisting}[language=Python]
def find_optimal_topics(corpus, dictionary, texts, min_topics=5, max_topics=20):
    """Find optimal number of topics using coherence."""
    coherence_scores = []

    for num_topics in range(min_topics, max_topics + 1):
        model = models.LdaModel(
            corpus=corpus,
            id2word=dictionary,
            num_topics=num_topics,
            random_state=42,
            passes=10,
            alpha='auto'
        )

        coherence_model = CoherenceModel(
            model=model,
            texts=texts,
            dictionary=dictionary,
            coherence='c_v'
        )

        coherence = coherence_model.get_coherence()
        coherence_scores.append((num_topics, coherence))
        print(f"Topics: {num_topics}, Coherence: {coherence:.3f}")

    # Find best number
    best = max(coherence_scores, key=lambda x: x[1])
    print(f"\nOptimal number of topics: {best[0]} (coherence: {best[1]:.3f})")

    return coherence_scores

# Run optimization
scores = find_optimal_topics(corpus, dictionary, processed_docs)
\end{lstlisting}

\subsubsection{Step 6: Visualize Topics}\label{step-6-visualize-topics}

\begin{lstlisting}[language=Python]
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis

# Create interactive visualization
vis = gensimvis.prepare(lda_model, corpus, dictionary)

# Save as HTML
pyLDAvis.save_html(vis, 'lda_visualization.html')
print("Visualization saved as 'lda_visualization.html'")

# Display in Jupyter notebook
# pyLDAvis.display(vis)
\end{lstlisting}

\subsection{Advanced Techniques}\label{advanced-techniques}

\subsubsection{1. Online Learning (for large
datasets)}\label{online-learning-for-large-datasets}

\begin{lstlisting}[language=Python]
# For streaming data or very large corpora
lda_online = models.LdaModel(
    corpus=corpus,
    id2word=dictionary,
    num_topics=10,
    update_every=1,      # Update model every document
    chunksize=100,       # Process 100 documents at a time
    passes=1,            # Single pass for online learning
    alpha='auto'
)
\end{lstlisting}

\subsubsection{2. Domain-Specific
Stopwords}\label{domain-specific-stopwords}

\begin{lstlisting}[language=Python]
# Add domain-specific words to filter
domain_stopwords = {'product', 'item', 'thing', 'stuff'}
stop_words.update(domain_stopwords)
\end{lstlisting}

\subsubsection{3. Bigrams and Trigrams}\label{bigrams-and-trigrams}

\begin{lstlisting}[language=Python]
from gensim.models import Phrases

# Detect common phrases
bigram = Phrases(processed_docs, min_count=5, threshold=100)
bigram_mod = bigram.freeze()

# Apply to documents
processed_docs_bigrams = [bigram_mod[doc] for doc in processed_docs]
\end{lstlisting}

\subsection{Practical Tips}\label{practical-tips}

\subsubsection{Preprocessing Best
Practices}\label{preprocessing-best-practices}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Keep domain knowledge}: Don't remove important domain terms
\item
  \textbf{Balance filtering}: Too aggressive = loss of meaning
\item
  \textbf{Preserve phrases}: ``machine learning'' should stay together
\item
  \textbf{Consider POS tagging}: Keep only nouns and verbs
\end{enumerate}

\subsubsection{Parameter Tuning}\label{parameter-tuning}

\begin{itemize}
\tightlist
\item
  \textbf{Start with defaults}: num\_topics=10, alpha=`auto',
  beta=`auto'
\item
  \textbf{Use coherence}: Not perplexity for evaluation
\item
  \textbf{Grid search carefully}: Topics × alpha × beta = many
  combinations
\item
  \textbf{Validate with humans}: Coherence doesn't guarantee usefulness
\end{itemize}

\subsubsection{Common Pitfalls}\label{common-pitfalls}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Too few documents}: Need 100+ per expected topic
\item
  \textbf{Too many topics}: Overfitting, uninterpretable
\item
  \textbf{No preprocessing}: Garbage in, garbage out
\item
  \textbf{Ignoring coherence}: Random topics aren't useful
\item
  \textbf{Not iterating}: First model is rarely the best
\end{enumerate}

\subsection{Exercise: Build Your Own Topic
Model}\label{exercise-build-your-own-topic-model}

\subsubsection{Dataset}\label{dataset}

Use this product review dataset:

\begin{lstlisting}[language=Python]
reviews = [
    "Great battery life, lasts all day",
    "Beautiful design and premium feel",
    "Fast shipping and good packaging",
    # Add 20+ more reviews covering various aspects
]
\end{lstlisting}

\subsubsection{Tasks}\label{tasks}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Preprocess the reviews
\item
  Build an LDA model with 3-5 topics
\item
  Calculate coherence score
\item
  Interpret the topics
\item
  Find optimal number of topics
\end{enumerate}

\subsubsection{Expected Output}\label{expected-output}

\begin{itemize}
\tightlist
\item
  Topic 0: Battery/Power
\item
  Topic 1: Design/Aesthetics
\item
  Topic 2: Shipping/Service
\item
  Coherence \textgreater{} 0.4
\end{itemize}

\subsection{Next Steps}\label{next-steps}

\begin{itemize}
\tightlist
\item
  Try different preprocessing strategies
\item
  Experiment with NMF as alternative
\item
  Apply to your organization's data
\item
  Build a topic-based recommendation system
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\emph{Remember: Good topic modeling is iterative. Experiment, evaluate,
and refine.}

\end{document}
