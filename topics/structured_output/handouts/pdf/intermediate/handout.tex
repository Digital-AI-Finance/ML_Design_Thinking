% Pandoc LaTeX template for course handouts
% Usage: pandoc input.md -o output.tex --template=handout_template.tex --listings
\documentclass[10pt,a4paper]{article}

% Geometry
\usepackage[margin=2.5cm]{geometry}

% Fonts and encoding
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}

% Math
\usepackage{amsmath,amssymb,amsfonts}

% Tables
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}

% Graphics
\usepackage{graphicx}

% Colors
\usepackage{xcolor}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{codebg}{RGB}{248,248,248}
\definecolor{codeframe}{RGB}{200,200,200}

% Code listings
\usepackage{listings}
\lstset{
  backgroundcolor=\color{codebg},
  basicstyle=\ttfamily\small,
  breakatwhitespace=false,
  breaklines=true,
  captionpos=b,
  commentstyle=\color{gray},
  frame=single,
  framerule=0.5pt,
  rulecolor=\color{codeframe},
  keepspaces=true,
  keywordstyle=\color{mlblue}\bfseries,
  language=Python,
  numbers=none,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  stringstyle=\color{mlpurple},
  tabsize=4,
  xleftmargin=0.5em,
  xrightmargin=0.5em,
  aboveskip=1em,
  belowskip=1em
}

% Hyperlinks
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=mlblue,
  urlcolor=mlblue,
  citecolor=mlblue
}

% Headers and footers
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\lhead{\textcolor{mlpurple}{\textbf{Structured Output}}}
\rhead{\textcolor{gray}{Intermediate}}
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt}

% Paragraph spacing
\setlength{\parindent}{0pt}
\setlength{\parskip}{0.5em}

% Section formatting
\usepackage{titlesec}
\titleformat{\section}
  {\Large\bfseries\color{mlpurple}}
  {\thesection}{1em}{}
\titleformat{\subsection}
  {\large\bfseries\color{mlblue}}
  {\thesubsection}{1em}{}
\titleformat{\subsubsection}
  {\normalsize\bfseries}
  {\thesubsubsection}{1em}{}

% Lists
\usepackage{enumitem}
\setlist[itemize]{topsep=0.5em, itemsep=0.25em}
\setlist[enumerate]{topsep=0.5em, itemsep=0.25em}

% Tight list support for pandoc
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% Title
\title{\Huge\bfseries\color{mlpurple} Handout 2: Implementing Structured Output Systems}
\author{Machine Learning for Smarter Innovation}
\date{}

\begin{document}

\maketitle
\thispagestyle{fancy}

\section{Handout 2: Implementing Structured Output
Systems}\label{handout-2-implementing-structured-output-systems}

\subsection{Step-by-Step Implementation
Guide}\label{step-by-step-implementation-guide}

\subsubsection{Prerequisites}\label{prerequisites}

\begin{itemize}
\tightlist
\item
  Basic Python knowledge
\item
  OpenAI or Anthropic API key
\item
  Understanding of JSON format
\item
  Read Handout 1 first
\end{itemize}

\subsubsection{Part 1: Understanding Function
Calling}\label{part-1-understanding-function-calling}

Function calling is OpenAI's way of getting structured outputs. Instead
of asking the model to ``return JSON'', you define the exact structure
you want.

\paragraph{How It Works}\label{how-it-works}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  You define a function schema (what fields, what types)
\item
  AI decides if it should call the function
\item
  AI generates the arguments in JSON format
\item
  You receive structured, validated data
\end{enumerate}

\paragraph{Basic Example}\label{basic-example}

\begin{lstlisting}[language=Python]
import openai

# Define your function schema
functions = [
    {
        "name": "extract_review_data",
        "description": "Extract structured data from a restaurant review",
        "parameters": {
            "type": "object",
            "properties": {
                "rating": {
                    "type": "integer",
                    "description": "Overall rating from 1-5",
                    "minimum": 1,
                    "maximum": 5
                },
                "price_level": {
                    "type": "string",
                    "enum": ["cheap", "moderate", "expensive"],
                    "description": "Price category"
                },
                "food_quality": {
                    "type": "integer",
                    "minimum": 1,
                    "maximum": 5
                }
            },
            "required": ["rating", "price_level"]
        }
    }
]

# Call the API
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[
        {"role": "user", "content": "Review: The food was amazing! 5 stars. A bit pricey at $40 per person but worth it."}
    ],
    functions=functions,
    function_call={"name": "extract_review_data"}  # Force function call
)

# Extract the result
function_call = response.choices[0].message.function_call
arguments = json.loads(function_call.arguments)

print(arguments)
# Output: {"rating": 5, "price_level": "expensive", "food_quality": 5}
\end{lstlisting}

\subsubsection{Part 2: Pydantic for
Validation}\label{part-2-pydantic-for-validation}

Pydantic gives you type-safe Python with automatic validation.

\paragraph{Installation}\label{installation}

\begin{lstlisting}[language=bash]
pip install pydantic
\end{lstlisting}

\paragraph{Define Your Schema}\label{define-your-schema}

\begin{lstlisting}[language=Python]
from pydantic import BaseModel, Field, validator
from typing import List, Optional

class RestaurantReview(BaseModel):
    rating: int = Field(..., ge=1, le=5, description="Overall rating")
    food_quality: int = Field(..., ge=1, le=5)
    service_quality: int = Field(..., ge=1, le=5)
    price_level: str = Field(..., regex="^(cheap|moderate|expensive)$")
    avg_price_per_person: Optional[float] = Field(None, gt=0)
    themes: List[str] = Field(default_factory=list, max_items=5)
    recommended_for: List[str] = []

    @validator('price_level')
    def validate_price_level(cls, v):
        allowed = ['cheap', 'moderate', 'expensive']
        if v not in allowed:
            raise ValueError(f'Must be one of {allowed}')
        return v

    @validator('recommended_for')
    def validate_recommendations(cls, v):
        allowed = ['date', 'family', 'business', 'friends', 'solo']
        for item in v:
            if item not in allowed:
                raise ValueError(f'{item} not in allowed values')
        return v

# Use it
try:
    review = RestaurantReview(
        rating=5,
        food_quality=5,
        service_quality=4,
        price_level="moderate",
        themes=["italian", "romantic", "authentic"]
    )
    print(review.dict())  # Convert to dictionary
    print(review.json())  # Convert to JSON string
except ValidationError as e:
    print(f"Validation failed: {e}")
\end{lstlisting}

\subsubsection{Part 3: Complete
Implementation}\label{part-3-complete-implementation}

Here's a production-ready implementation:

\begin{lstlisting}[language=Python]
import openai
import json
import time
from pydantic import BaseModel, Field, ValidationError
from typing import Optional, List

# 1. Define Pydantic model
class ReviewData(BaseModel):
    rating: int = Field(..., ge=1, le=5)
    food_quality: int = Field(..., ge=1, le=5)
    service_quality: int = Field(..., ge=1, le=5)
    price_level: str
    confidence: Optional[float] = None

# 2. Define OpenAI function
functions = [{
    "name": "extract_review",
    "description": "Extract structured data from review",
    "parameters": ReviewData.schema()
}]

# 3. Extraction function with retry logic
def extract_review_data(review_text: str, max_retries: int = 3) -> Optional[ReviewData]:
    """Extract structured data from review with retry logic"""

    for attempt in range(max_retries):
        try:
            # Call OpenAI
            response = openai.ChatCompletion.create(
                model="gpt-4",
                temperature=0.1,  # Low temperature for consistency
                messages=[
                    {"role": "system", "content": "You are a data extraction expert. Extract information accurately."},
                    {"role": "user", "content": f"Extract data from this review:\n\n{review_text}"}
                ],
                functions=functions,
                function_call={"name": "extract_review"}
            )

            # Parse result
            function_call = response.choices[0].message.function_call
            data = json.loads(function_call.arguments)

            # Validate with Pydantic
            review_data = ReviewData(**data)

            return review_data

        except ValidationError as e:
            print(f"Validation error on attempt {attempt + 1}: {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # Exponential backoff
                continue
            else:
                return None

        except Exception as e:
            print(f"API error on attempt {attempt + 1}: {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
                continue
            else:
                return None

    return None

# 4. Use it
review_text = """
The food was absolutely amazing! Best Italian I've had in years.
Service was friendly and attentive. A bit pricey at $45 per person
but definitely worth it for a special occasion.
"""

result = extract_review_data(review_text)

if result:
    print("Success!")
    print(result.json(indent=2))
else:
    print("Extraction failed after all retries")
\end{lstlisting}

\subsubsection{Part 4: Error Handling
Patterns}\label{part-4-error-handling-patterns}

\paragraph{Pattern 1: Graceful
Degradation}\label{pattern-1-graceful-degradation}

\begin{lstlisting}[language=Python]
def extract_with_fallback(text: str) -> dict:
    """Try AI extraction, fall back to rule-based"""

    # Try AI first
    result = extract_with_ai(text)
    if result and validate(result):
        return result

    # Fallback to rule-based
    print("AI failed, using rule-based extraction")
    return rule_based_extraction(text)

def rule_based_extraction(text: str) -> dict:
    """Simple keyword-based extraction as fallback"""
    import re

    # Extract rating with regex
    rating_match = re.search(r'(\d+)\s*stars?|\b(one|two|three|four|five)\b', text.lower())

    # Extract price indicators
    price_match = re.search(r'\$(\d+)', text)

    return {
        "rating": extract_rating(rating_match) if rating_match else 3,
        "price_level": estimate_price(price_match) if price_match else "moderate",
        "confidence": 0.5  # Low confidence for rule-based
    }
\end{lstlisting}

\paragraph{Pattern 2: Validation
Layers}\label{pattern-2-validation-layers}

\begin{lstlisting}[language=Python]
def multi_stage_validation(data: dict) -> bool:
    """Three-stage validation"""

    # Stage 1: Schema validation
    try:
        review = ReviewData(**data)
    except ValidationError:
        return False

    # Stage 2: Business rules
    if review.rating == 5 and review.service_quality < 3:
        # Suspicious: 5-star overall but poor service?
        return False

    if review.price_level == "cheap" and review.avg_price_per_person > 30:
        # Inconsistent: cheap but $30+?
        return False

    # Stage 3: Confidence check
    if hasattr(review, 'confidence') and review.confidence < 0.7:
        # Low confidence, needs human review
        return False

    return True
\end{lstlisting}

\subsubsection{Part 5: Testing Strategy}\label{part-5-testing-strategy}

\paragraph{Unit Tests}\label{unit-tests}

\begin{lstlisting}[language=Python]
import pytest

def test_schema_validation():
    """Test Pydantic schema catches invalid data"""

    # Valid data
    valid = ReviewData(
        rating=5,
        food_quality=5,
        service_quality=4,
        price_level="moderate"
    )
    assert valid.rating == 5

    # Invalid rating
    with pytest.raises(ValidationError):
        ReviewData(
            rating=6,  # Too high!
            food_quality=5,
            service_quality=4,
            price_level="moderate"
        )

    # Invalid price level
    with pytest.raises(ValidationError):
        ReviewData(
            rating=5,
            food_quality=5,
            service_quality=4,
            price_level="super_expensive"  # Not in enum!
        )
\end{lstlisting}

\paragraph{Integration Tests}\label{integration-tests}

\begin{lstlisting}[language=Python]
def test_full_pipeline():
    """Test complete extraction pipeline"""

    test_review = """
    Amazing restaurant! Food quality is top-notch, 5 stars.
    Service was excellent. Price is moderate around $25 per person.
    """

    result = extract_review_data(test_review)

    assert result is not None
    assert result.rating == 5
    assert result.food_quality == 5
    assert result.price_level == "moderate"
\end{lstlisting}

\subsubsection{Part 6: Production
Deployment}\label{part-6-production-deployment}

\paragraph{Environment Setup}\label{environment-setup}

\begin{lstlisting}[language=Python]
import os
from dotenv import load_dotenv

# Load API keys
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# Configuration
CONFIG = {
    "model": "gpt-4",
    "temperature": 0.1,
    "max_tokens": 500,
    "timeout": 30,
    "max_retries": 3
}
\end{lstlisting}

\paragraph{Logging}\label{logging}

\begin{lstlisting}[language=Python]
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def extract_with_logging(text: str) -> Optional[ReviewData]:
    """Extract with comprehensive logging"""

    logger.info(f"Starting extraction for text of length {len(text)}")

    try:
        result = extract_review_data(text)

        if result:
            logger.info("Extraction successful")
            logger.debug(f"Extracted data: {result.json()}")
        else:
            logger.warning("Extraction failed after all retries")

        return result

    except Exception as e:
        logger.error(f"Unexpected error: {e}", exc_info=True)
        return None
\end{lstlisting}

\paragraph{Monitoring}\label{monitoring}

\begin{lstlisting}[language=Python]
from datetime import datetime

class MetricsCollector:
    def __init__(self):
        self.successes = 0
        self.failures = 0
        self.total_time = 0

    def record_success(self, duration: float):
        self.successes += 1
        self.total_time += duration

    def record_failure(self):
        self.failures += 1

    def get_metrics(self) -> dict:
        total = self.successes + self.failures
        return {
            "success_rate": self.successes / total if total > 0 else 0,
            "failure_rate": self.failures / total if total > 0 else 0,
            "avg_duration": self.total_time / self.successes if self.successes > 0 else 0,
            "total_processed": total
        }

# Use it
metrics = MetricsCollector()

def extract_with_metrics(text: str) -> Optional[ReviewData]:
    start_time = time.time()

    result = extract_review_data(text)

    duration = time.time() - start_time

    if result:
        metrics.record_success(duration)
    else:
        metrics.record_failure()

    return result
\end{lstlisting}

\subsubsection{Part 7: Common Issues and
Solutions}\label{part-7-common-issues-and-solutions}

\paragraph{Issue 1: Inconsistent
Outputs}\label{issue-1-inconsistent-outputs}

\textbf{Problem:} Same input gives different outputs

\textbf{Solution:} - Set temperature to 0 - Use deterministic model -
Add more examples to prompt - Use function calling instead of raw JSON

\paragraph{Issue 2: Slow Performance}\label{issue-2-slow-performance}

\textbf{Problem:} Takes 5+ seconds per request

\textbf{Solution:}

\begin{lstlisting}[language=Python]
# Use caching
from functools import lru_cache

@lru_cache(maxsize=1000)
def extract_cached(text: str) -> dict:
    """Cache results for identical inputs"""
    return extract_review_data(text)

# Use smaller model for simple tasks
CONFIG = {
    "model": "gpt-3.5-turbo",  # Faster than gpt-4
    "temperature": 0.1
}
\end{lstlisting}

\paragraph{Issue 3: High API Costs}\label{issue-3-high-api-costs}

\textbf{Problem:} Spending \$500/month on API calls

\textbf{Solution:} - Cache aggressively - Use smaller models when
possible - Batch requests - Reduce prompt length - Remove unnecessary
examples after tuning

\subsubsection{Part 8: Next Steps}\label{part-8-next-steps}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Build the workshop project} - Restaurant review extraction
\item
  \textbf{Add monitoring} - Track success rates in production
\item
  \textbf{Optimize costs} - Implement caching and batching
\item
  \textbf{Read Handout 3} - Advanced techniques for 99\%+ reliability
\end{enumerate}

\subsubsection{Key Takeaways}\label{key-takeaways}

\begin{itemize}
\tightlist
\item
  Function calling + Pydantic = 95\%+ reliability
\item
  Always implement retry logic with exponential backoff
\item
  Test with real data, not just happy paths
\item
  Monitor everything in production
\item
  Have fallback strategies for failures
\item
  Start simple, add complexity gradually
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\emph{Next: Handout 3 for production-grade advanced techniques}

\end{document}
