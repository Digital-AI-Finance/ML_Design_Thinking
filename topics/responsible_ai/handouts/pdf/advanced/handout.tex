% Pandoc LaTeX template for course handouts
% Usage: pandoc input.md -o output.tex --template=handout_template.tex --listings
\documentclass[10pt,a4paper]{article}

% Geometry
\usepackage[margin=2.5cm]{geometry}

% Fonts and encoding
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}

% Math
\usepackage{amsmath,amssymb,amsfonts}

% Tables
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}

% Graphics
\usepackage{graphicx}

% Colors
\usepackage{xcolor}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{codebg}{RGB}{248,248,248}
\definecolor{codeframe}{RGB}{200,200,200}

% Code listings
\usepackage{listings}
\lstset{
  backgroundcolor=\color{codebg},
  basicstyle=\ttfamily\small,
  breakatwhitespace=false,
  breaklines=true,
  captionpos=b,
  commentstyle=\color{gray},
  frame=single,
  framerule=0.5pt,
  rulecolor=\color{codeframe},
  keepspaces=true,
  keywordstyle=\color{mlblue}\bfseries,
  language=Python,
  numbers=none,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  stringstyle=\color{mlpurple},
  tabsize=4,
  xleftmargin=0.5em,
  xrightmargin=0.5em,
  aboveskip=1em,
  belowskip=1em
}

% Hyperlinks
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=mlblue,
  urlcolor=mlblue,
  citecolor=mlblue
}

% Headers and footers
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\lhead{\textcolor{mlpurple}{\textbf{Responsible AI}}}
\rhead{\textcolor{gray}{Advanced}}
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt}

% Paragraph spacing
\setlength{\parindent}{0pt}
\setlength{\parskip}{0.5em}

% Section formatting
\usepackage{titlesec}
\titleformat{\section}
  {\Large\bfseries\color{mlpurple}}
  {\thesection}{1em}{}
\titleformat{\subsection}
  {\large\bfseries\color{mlblue}}
  {\thesubsection}{1em}{}
\titleformat{\subsubsection}
  {\normalsize\bfseries}
  {\thesubsubsection}{1em}{}

% Lists
\usepackage{enumitem}
\setlist[itemize]{topsep=0.5em, itemsep=0.25em}
\setlist[enumerate]{topsep=0.5em, itemsep=0.25em}

% Tight list support for pandoc
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% Title
\title{\Huge\bfseries\color{mlpurple} Handout 3: Advanced Fairness Metrics}
\author{Machine Learning for Smarter Innovation}
\date{}

\begin{document}

\maketitle
\thispagestyle{fancy}

\section{Handout 3: Advanced Fairness
Metrics}\label{handout-3-advanced-fairness-metrics}

\subsection{Mathematical Foundations \&
Implementation}\label{mathematical-foundations-implementation}

\subsubsection{Part 1: Group Fairness
Metrics}\label{part-1-group-fairness-metrics}

\paragraph{1.1 Demographic Parity (Statistical
Parity)}\label{demographic-parity-statistical-parity}

\textbf{Definition:}

\begin{lstlisting}
P(D = 1 | A = a) = P(D = 1 | A = b) for all a, b
\end{lstlisting}

Where: - D: Decision (1 = positive outcome) - A: Protected attribute

\textbf{Relaxed Version} (ε-demographic parity):

\begin{lstlisting}
|P(D = 1 | A = a) - P(D = 1 | A = b)| ≤ ε
\end{lstlisting}

Typical threshold: ε = 0.1 (10\% disparity allowed)

\textbf{Pros:} - Simple to measure and explain - Ensures equal access to
opportunities - No need for ground truth labels

\textbf{Cons:} - Ignores base rate differences - Can reduce overall
accuracy - May require different error rates by group

\textbf{Implementation:}

\begin{lstlisting}[language=Python]
def demographic_parity_difference(y_true, y_pred, sensitive_features):
    groups = np.unique(sensitive_features)
    rates = []

    for group in groups:
        mask = sensitive_features == group
        rate = np.mean(y_pred[mask])
        rates.append(rate)

    return max(rates) - min(rates)

# Alternative: Demographic parity ratio
def demographic_parity_ratio(y_true, y_pred, sensitive_features):
    groups = np.unique(sensitive_features)
    rates = []

    for group in groups:
        mask = sensitive_features == group
        rate = np.mean(y_pred[mask])
        rates.append(rate)

    return min(rates) / max(rates)  # Ideal: 1.0, Acceptable: > 0.8
\end{lstlisting}

\paragraph{1.2 Equal Opportunity}\label{equal-opportunity}

\textbf{Definition:}

\begin{lstlisting}
P(D = 1 | Y = 1, A = a) = P(D = 1 | Y = 1, A = b)
\end{lstlisting}

Equivalently: TPR\_a = TPR\_b

\textbf{Intuition}: Qualified individuals have equal chances regardless
of group.

\textbf{Mathematical Formulation:}

\begin{lstlisting}
TPR_a = TP_a / (TP_a + FN_a)
TPR_b = TP_b / (TP_b + FN_b)

Equal Opportunity Difference = |TPR_a - TPR_b|
\end{lstlisting}

\textbf{Pros:} - Focuses on qualified individuals - Allows different
base rates - Prevents discrimination against deserving

\textbf{Cons:} - Ignores false positives - Requires ground truth labels
- May not prevent all harms

\textbf{Implementation:}

\begin{lstlisting}[language=Python]
def equal_opportunity_difference(y_true, y_pred, sensitive_features):
    groups = np.unique(sensitive_features)
    tprs = []

    for group in groups:
        mask = (sensitive_features == group) & (y_true == 1)
        if mask.sum() > 0:
            tpr = np.mean(y_pred[mask])
            tprs.append(tpr)

    return max(tprs) - min(tprs) if tprs else 0.0
\end{lstlisting}

\paragraph{1.3 Equalized Odds}\label{equalized-odds}

\textbf{Definition:}

\begin{lstlisting}
P(D = 1 | Y = y, A = a) = P(D = 1 | Y = y, A = b) for all y ∈ {0, 1}
\end{lstlisting}

Equivalently: TPR\_a = TPR\_b AND FPR\_a = FPR\_b

\textbf{Mathematical Formulation:}

\begin{lstlisting}
TPR: P(D = 1 | Y = 1, A = a) = P(D = 1 | Y = 1, A = b)
FPR: P(D = 1 | Y = 0, A = a) = P(D = 1 | Y = 0, A = b)

Equalized Odds Difference = max(|TPR_a - TPR_b|, |FPR_a - FPR_b|)
\end{lstlisting}

\textbf{Pros:} - Strongest group fairness notion - Balances both types
of errors - Prediction independent of group given label

\textbf{Cons:} - Most restrictive (hardest to achieve) - May sacrifice
accuracy - Requires careful calibration

\textbf{Implementation:}

\begin{lstlisting}[language=Python]
def equalized_odds_difference(y_true, y_pred, sensitive_features):
    groups = np.unique(sensitive_features)
    tprs, fprs = [], []

    for group in groups:
        # TPR
        mask_pos = (sensitive_features == group) & (y_true == 1)
        if mask_pos.sum() > 0:
            tpr = np.mean(y_pred[mask_pos])
            tprs.append(tpr)

        # FPR
        mask_neg = (sensitive_features == group) & (y_true == 0)
        if mask_neg.sum() > 0:
            fpr = np.mean(y_pred[mask_neg])
            fprs.append(fpr)

    tpr_diff = max(tprs) - min(tprs) if tprs else 0.0
    fpr_diff = max(fprs) - min(fprs) if fprs else 0.0

    return max(tpr_diff, fpr_diff)
\end{lstlisting}

\paragraph{1.4 Calibration}\label{calibration}

\textbf{Definition:}

\begin{lstlisting}
P(Y = 1 | S(X) = s, A = a) = P(Y = 1 | S(X) = s, A = b) = s
\end{lstlisting}

Where S(X) is the model's predicted probability.

\textbf{Intuition}: A score of 0.7 means 70\% chance of positive
outcome, regardless of group.

\textbf{Pros:} - Important for decision-making under uncertainty -
Allows different acceptance rates - Interpretable probabilities

\textbf{Cons:} - Can coexist with other biases - Requires probability
predictions - May not ensure equal treatment

\textbf{Implementation:}

\begin{lstlisting}[language=Python]
from sklearn.calibration import calibration_curve

def calibration_by_group(y_true, y_pred_proba, sensitive_features, n_bins=10):
    groups = np.unique(sensitive_features)

    for group in groups:
        mask = sensitive_features == group
        prob_true, prob_pred = calibration_curve(
            y_true[mask],
            y_pred_proba[mask],
            n_bins=n_bins
        )

        # Calibration error
        calib_error = np.mean(np.abs(prob_true - prob_pred))
        print(f"Group {group}: Calibration Error = {calib_error:.4f}")

    # Cross-group calibration difference
    # Ideal: All groups have similar calibration curves
\end{lstlisting}

\subsubsection{Part 2: Individual
Fairness}\label{part-2-individual-fairness}

\paragraph{2.1 Lipschitz Condition}\label{lipschitz-condition}

\textbf{Definition:}

\begin{lstlisting}
d_Y(M(x_i), M(x_j)) ≤ L · d_X(x_i, x_j)
\end{lstlisting}

Where: - M: Model - d\_X: Distance metric in input space - d\_Y:
Distance metric in output space - L: Lipschitz constant

\textbf{Intuition}: Similar individuals should receive similar
predictions.

\textbf{Challenge}: Defining ``similar'' requires domain knowledge.

\textbf{Implementation:}

\begin{lstlisting}[language=Python]
def check_individual_fairness(model, X, distance_metric, k=10, threshold=0.1):
    from sklearn.neighbors import NearestNeighbors

    # Find k-nearest neighbors
    nbrs = NearestNeighbors(n_neighbors=k, metric=distance_metric)
    nbrs.fit(X)
    distances, indices = nbrs.kneighbors(X)

    # Get predictions
    y_pred = model.predict_proba(X)[:, 1]

    # Check prediction similarity for neighbors
    violations = []
    for i in range(len(X)):
        for j in range(1, k):  # Skip self (j=0)
            neighbor_idx = indices[i, j]
            input_dist = distances[i, j]
            output_dist = abs(y_pred[i] - y_pred[neighbor_idx])

            # Violation if output distance disproportionate to input distance
            if output_dist > threshold and input_dist < threshold:
                violations.append((i, neighbor_idx, input_dist, output_dist))

    return violations
\end{lstlisting}

\subsubsection{Part 3: Causal Fairness}\label{part-3-causal-fairness}

\paragraph{3.1 Counterfactual Fairness}\label{counterfactual-fairness}

\textbf{Definition:}

\begin{lstlisting}
P(Y_A←a(U) = y | X = x, A = a) = P(Y_A←a'(U) = y | X = x, A = a)
\end{lstlisting}

\textbf{Intuition}: Changing only the protected attribute should not
change the prediction.

\textbf{Requires}: Causal graph and structural equation model.

\textbf{Implementation} (conceptual):

\begin{lstlisting}[language=Python]
# Requires causal inference library (e.g., dowhy)
import dowhy

# 1. Define causal model
model = dowhy.CausalModel(
    data=df,
    treatment='protected_attribute',
    outcome='prediction',
    graph=causal_graph
)

# 2. Estimate counterfactual effect
identified_estimand = model.identify_effect()
estimate = model.estimate_effect(identified_estimand)

# 3. Check if effect is near zero
print(f"Causal effect: {estimate.value}")
# Ideal: ≈ 0
\end{lstlisting}

\subsubsection{Part 4: Impossibility
Theorems}\label{part-4-impossibility-theorems}

\paragraph{4.1 Chouldechova's Theorem
(2017)}\label{chouldechovas-theorem-2017}

\textbf{Statement}: If prevalence (base rates) differ across groups, you
cannot simultaneously achieve: 1. Calibration by group 2. Equal FPR 3.
Equal FNR

\textbf{Proof sketch}:

\begin{lstlisting}
Given:
- Prevalence_a ≠ Prevalence_b
- Calibration: PPV_a = PPV_b and NPV_a = NPV_b

Then:
- FPR and FNR must differ to maintain calibration

Mathematical derivation:
PPV = TP / (TP + FP) = TPR × Prev / (TPR × Prev + FPR × (1 - Prev))

If PPV_a = PPV_b and Prev_a ≠ Prev_b,
then TPR and FPR cannot both be equal across groups.
\end{lstlisting}

\paragraph{4.2 Kleinberg-Mullainathan-Raghavan
(2016)}\label{kleinberg-mullainathan-raghavan-2016}

\textbf{Statement}: Cannot simultaneously achieve: 1. Calibration 2.
Balance for positive class (equal opportunity) 3. Balance for negative
class (equal FPR)

Unless: Perfect prediction OR equal base rates

\textbf{Implication}: Must choose which fairness notion to prioritize
based on context.

\subsubsection{Part 5: Practical
Trade-offs}\label{part-5-practical-trade-offs}

\paragraph{5.1 Accuracy-Fairness
Trade-off}\label{accuracy-fairness-trade-off}

\textbf{Pareto Frontier}:

\begin{lstlisting}[language=Python]
from sklearn.model_selection import ParameterGrid

# Sweep fairness constraint strength
results = []
for constraint_weight in np.linspace(0, 1, 20):
    model = train_with_fairness_constraint(X_train, y_train, A_train, constraint_weight)

    accuracy = accuracy_score(y_test, model.predict(X_test))
    fairness = equalized_odds_difference(y_test, model.predict(X_test), A_test)

    results.append({
        'constraint_weight': constraint_weight,
        'accuracy': accuracy,
        'fairness_violation': fairness
    })

# Plot Pareto frontier
plt.scatter([r['fairness_violation'] for r in results],
            [r['accuracy'] for r in results])
plt.xlabel('Fairness Violation')
plt.ylabel('Accuracy')
plt.title('Accuracy-Fairness Trade-off')
\end{lstlisting}

\paragraph{5.2 Multi-objective
Optimization}\label{multi-objective-optimization}

\textbf{Formulation}:

\begin{lstlisting}
minimize: α × L(θ) + (1-α) × F(θ)

Where:
- L(θ): Loss function (e.g., cross-entropy)
- F(θ): Fairness violation
- α ∈ [0, 1]: Trade-off parameter
\end{lstlisting}

\textbf{Implementation}:

\begin{lstlisting}[language=Python]
import torch
import torch.nn as nn

class FairClassifier(nn.Module):
    def __init__(self, input_dim, alpha=0.5):
        super().__init__()
        self.model = nn.Linear(input_dim, 1)
        self.alpha = alpha

    def forward(self, x):
        return torch.sigmoid(self.model(x))

    def compute_loss(self, x, y, a):
        pred = self.forward(x)

        # Standard loss
        bce_loss = nn.BCELoss()(pred, y)

        # Fairness loss (demographic parity)
        groups = torch.unique(a)
        group_preds = [pred[a == g].mean() for g in groups]
        fairness_loss = torch.var(torch.tensor(group_preds))

        # Combined loss
        total_loss = self.alpha * bce_loss + (1 - self.alpha) * fairness_loss

        return total_loss
\end{lstlisting}

\subsubsection{Part 6: Advanced Mitigation
Techniques}\label{part-6-advanced-mitigation-techniques}

\paragraph{6.1 Adversarial Debiasing}\label{adversarial-debiasing}

\textbf{Formulation}:

\begin{lstlisting}
min_θ_P max_θ_A L_P(θ_P) - λ L_A(θ_A)

Where:
- θ_P: Predictor parameters
- θ_A: Adversary parameters
- L_P: Prediction loss
- L_A: Adversary loss (predict protected attribute from predictions)
- λ: Trade-off parameter
\end{lstlisting}

\textbf{Implementation}:

\begin{lstlisting}[language=Python]
class AdversarialDebiasing(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.predictor = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
        self.adversary = nn.Sequential(
            nn.Linear(1, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )

    def train_step(self, x, y, a, lambda_adv=1.0):
        # Train predictor
        pred = self.predictor(x)
        pred_loss = nn.BCELoss()(pred, y)

        # Train adversary (predict protected attribute from predictions)
        adv_pred = self.adversary(pred.detach())
        adv_loss = nn.BCELoss()(adv_pred, a)

        # Combined loss
        total_loss = pred_loss - lambda_adv * adv_loss

        return total_loss
\end{lstlisting}

\paragraph{6.2 Fairness Through Unawareness (and why it
fails)}\label{fairness-through-unawareness-and-why-it-fails}

\textbf{Naive Approach}: Remove protected attribute from features.

\textbf{Why it fails}: Proxy variables (correlated features) leak
information.

\textbf{Example}:

\begin{lstlisting}[language=Python]
# Even without explicit race,these features correlate:
- Zip code → residential segregation
- First name → cultural background
- School name → socioeconomic status

# Correlation analysis
for feature in features:
    corr = np.corrcoef(df[feature], df['race'])[0, 1]
    if abs(corr) > 0.3:
        print(f"{feature}: {corr:.3f} correlation with race")
\end{lstlisting}

\textbf{Proper Approach}: Use fairness-aware methods that account for
correlation structure.

\subsubsection{Key Takeaway}\label{key-takeaway}

\textbf{Mathematics provides precise definitions, but choosing the right
fairness metric requires ethical judgment, domain knowledge, and
stakeholder input.}

Different contexts require different fairness notions - there is no
universal solution.

\end{document}
