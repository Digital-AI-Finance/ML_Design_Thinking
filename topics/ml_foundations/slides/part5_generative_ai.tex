% Part 5: Generative AI and Modern Applications
\section{Generative AI and Modern Applications}

% Section divider
\begin{frame}[plain]
\vfill
\centering
\begin{beamercolorbox}[sep=16pt,center]{title}
\usebeamerfont{title}\Large Part 5: Generative AI and Modern Applications\par
\vspace{0.5em}
\large Creating New Content with Artificial Intelligence\par
\end{beamercolorbox}
\vfill
\end{frame}

% Generative vs Discriminative
\begin{frame}{Generative vs Discriminative Models}
\twocolslide{
\Large\textbf{Discriminative Models}
\normalsize
\vspace{0.5em}

\textbf{Goal:} Learn decision boundary
\formula{p(y|x) = \frac{1}{1 + e^{-f(x)}}}

\textbf{Examples:}
\begin{itemize}
\item Logistic regression
\item Support Vector Machines
\item Neural network classifiers
\item Decision trees
\end{itemize}

\textbf{Applications:}
\begin{itemize}
\item Classification tasks
\item Regression problems
\item Prediction from features
\item Pattern recognition
\end{itemize}

\textbf{Advantages:}
\begin{itemize}
\item Often better at classification
\item More direct approach
\item Typically faster training
\end{itemize}
}{
\Large\textbf{Generative Models}
\normalsize
\vspace{0.5em}

\textbf{Goal:} Learn data distribution
\formula{p(x) \text{ or } p(x,y)}

\textbf{Examples:}
\begin{itemize}
\item Generative Adversarial Networks
\item Variational Autoencoders
\item Autoregressive models
\item Diffusion models
\end{itemize}

\textbf{Applications:}
\begin{itemize}
\item Data generation
\item Image synthesis
\item Text generation
\item Data augmentation
\end{itemize}

\textbf{Advantages:}
\begin{itemize}
\item Can generate new samples
\item Handle missing data
\item Provide uncertainty estimates
\end{itemize}
}

\bottomnote{Generative models learn data distributions - probabilistic frameworks enable sampling and density estimation beyond discriminative classification}
\end{frame}

% Generative Adversarial Networks
\begin{frame}{Generative Adversarial Networks: The Two-Player Game}
\twocolslide{
\Large\textbf{Mathematical Framework}
\normalsize
\vspace{0.5em}

\textbf{Generator:} $G: \mathcal{Z} \rightarrow \mathcal{X}$
\formula{G(z) \text{ where } z \sim p_z(z)}

\textbf{Discriminator:} $D: \mathcal{X} \rightarrow [0,1]$
\formula{D(x) = \text{Probability } x \text{ is real}}

\textbf{Minimax Objective:}
\formula{\min_G \max_D V(D,G)}

where:
\formula{V(D,G) = \mathbb{E}_{x \sim p_{data}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log(1-D(G(z)))]}

\textbf{Training Process:}
\begin{enumerate}
\item Train D to distinguish real vs fake
\item Train G to fool D
\item Alternate until convergence
\end{enumerate}
}{
\centering
\includegraphics[width=0.85\textwidth]{charts/gan_architecture.pdf}

\vspace{0.5em}
\textbf{Training Challenges:}
\begin{itemize}
\item Mode collapse
\item Training instability
\item Vanishing gradients
\item Nash equilibrium difficult to reach
\end{itemize}

\textbf{GAN Variants:}
\begin{itemize}
\item DCGAN: Deep Convolutional GANs
\item StyleGAN: Style-based generation
\item CycleGAN: Unpaired image translation
\item BigGAN: Large-scale high-quality images
\end{itemize}
}

\bottomnote{Adversarial training creates realistic samples - minimax game drives generator and discriminator toward Nash equilibrium}
\end{frame}

% Variational Autoencoders
\begin{frame}{Variational Autoencoders: Probabilistic Generation}
\twocolslide{
\Large\textbf{Probabilistic Framework}
\normalsize
\vspace{0.5em}

\textbf{Encoder (Recognition Model):}
\formula{q_\phi(z|x) \approx p(z|x)}

\textbf{Decoder (Generative Model):}
\formula{p_\theta(x|z)}

\textbf{Evidence Lower Bound (ELBO):}
\formula{\log p(x) \geq \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - KL(q_\phi(z|x)||p(z))}

\textbf{Loss Function:}
\formula{\mathcal{L} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - \beta \cdot KL(q_\phi(z|x)||p(z))}

\textbf{Reparameterization Trick:}
\formula{z = \mu + \sigma \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0,I)}

Enables backpropagation through stochastic node
}{
\centering
\includegraphics[width=0.85\textwidth]{charts/vae_architecture.pdf}

\vspace{0.5em}
\textbf{Key Advantages:}
\begin{itemize}
\item Stable training
\item Meaningful latent space
\item Principled probabilistic approach
\item Good reconstruction quality
\end{itemize}

\textbf{Applications:}
\begin{itemize}
\item Image generation
\item Data compression
\item Anomaly detection
\item Latent space interpolation
\end{itemize}

\textbf{VAE Variants:}
\begin{itemize}
\item $\beta$-VAE: Disentangled representations
\item WAE: Wasserstein Autoencoders
\end{itemize}
}

\bottomnote{Variational autoencoders combine encoding with regularization - probabilistic latent space enables principled sampling and interpolation}
\end{frame}

% Diffusion Models
\begin{frame}{Diffusion Models: Iterative Denoising Generation}
\twocolslide{
\Large\textbf{Mathematical Formulation}
\normalsize
\vspace{0.5em}

\textbf{Forward Process (Noise Addition):}
\formula{q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I)}

\textbf{Reverse Process (Denoising):}
\formula{p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t,t), \Sigma_\theta(x_t,t))}

\textbf{Training Objective:}
\formula{L = \mathbb{E}_{t,x_0,\epsilon}[||\epsilon - \epsilon_\theta(x_t,t)||^2]}

where $\epsilon_\theta$ predicts noise added at step $t$

\textbf{Sampling Process:}
\begin{enumerate}
\item Start with random noise $x_T \sim \mathcal{N}(0,I)$
\item Iteratively denoise: $x_{t-1} = \mu_\theta(x_t,t) + \sigma_t \epsilon$
\item Continue until $x_0$ (clean sample)
\end{enumerate}
}{
\centering
\includegraphics[width=0.85\textwidth]{charts/diffusion_process.pdf}

\vspace{0.5em}
\textbf{Key Properties:}
\begin{itemize}
\item High-quality generation
\item Stable training
\item Flexible conditioning
\item Controllable generation process
\end{itemize}

\textbf{Applications:}
\begin{itemize}
\item Image synthesis (DALL-E 2)
\item Video generation
\item Audio synthesis
\item 3D generation
\end{itemize}

\textbf{Variants:}
\begin{itemize}
\item DDPM: Denoising Diffusion Probabilistic Models
\item DDIM: Deterministic sampling
\end{itemize}
}

\bottomnote{Diffusion models reverse corruption iteratively - denoising process generates high-quality samples through gradual refinement}
\end{frame}

% Transformer Architecture
\begin{frame}{Transformers: The Foundation of Modern AI}
\twocolslide{
\Large\textbf{Self-Attention Mechanism}
\normalsize
\vspace{0.5em}

\textbf{Attention Formula:}
\formula{\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V}

where:
\begin{itemize}
\item $Q$: Queries matrix
\item $K$: Keys matrix
\item $V$: Values matrix
\item $d_k$: Dimension of keys
\end{itemize}

\textbf{Multi-Head Attention:}
\formula{\text{MultiHead}(Q,K,V) = \text{Concat}(head_1, \ldots, head_h)W^O}

where:
\formula{head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)}

\textbf{Position Encoding:} Since no recurrence
\formula{PE_{(pos,2i)} = \sin(pos/10000^{2i/d_{model}})}
\formula{PE_{(pos,2i+1)} = \cos(pos/10000^{2i/d_{model}})}
}{
\centering
\includegraphics[width=0.85\textwidth]{charts/transformer_architecture.pdf}

\vspace{0.5em}
\textbf{Architecture Components:}
\begin{itemize}
\item \textbf{Encoder:} Self-attention + Feed-forward
\item \textbf{Decoder:} Masked self-attention + Cross-attention
\item \textbf{Layer Norm:} Stabilizes training
\item \textbf{Residual Connections:} Gradient flow
\end{itemize}

\textbf{Key Advantages:}
\begin{itemize}
\item Parallelizable training
\item Long-range dependencies
\item Transfer learning capability
\item Attention interpretability
\end{itemize}
}

\bottomnote{Self-attention mechanisms capture long-range dependencies - parallel processing and position encoding enable scalable sequence modeling}
\end{frame}

% Large Language Models
\begin{frame}{Large Language Models: Scaling Language AI}
\twocolslide{
\textbf{GPT (Generative Pre-trained Transformer):}
\small
\begin{itemize}
\item Autoregressive generation
\item Transformer decoder architecture
\end{itemize}
\normalsize

\textbf{BERT (Bidirectional):}
\small
\begin{itemize}
\item Bidirectional context
\item Masked language modeling
\end{itemize}
\normalsize

\textbf{Scaling Laws:}
\formula{L(N) = \left(\frac{N_c}{N}\right)^{\alpha}}

\small Key findings: performance scales predictably, emergent abilities at scale
}{
\centering
\includegraphics[width=0.75\textwidth]{charts/llm_evolution.pdf}

\small
\textbf{Model Sizes:}
\begin{itemize}
\item GPT-1: 117M (2018), GPT-2: 1.5B (2019)
\item GPT-3: 175B (2020), GPT-4: ~1.8T (2023)
\end{itemize}

\textbf{Capabilities:} Text generation, QA, code, reasoning

\textbf{Training:} Pre-training + fine-tuning + RLHF
}

\bottomnote{Large language models exhibit emergent capabilities at scale - pre-training on massive corpora enables few-shot learning and reasoning}
\end{frame}

% Modern Applications
\begin{frame}{Generative AI Applications: Transforming Industries}
\small
\begin{columns}[T]
\begin{column}{0.32\textwidth}
\centering
\textcolor{chartblue}{\textbf{Content Creation}}

\textbf{Text:} GPT-4, Claude, Jasper

\textbf{Image:} DALL-E, Midjourney, Stable Diffusion

\textbf{Video:} Runway, Synthesia
\end{column}

\begin{column}{0.32\textwidth}
\centering
\textcolor{chartteal}{\textbf{Code \& Development}}

\textbf{Generation:} GitHub Copilot, AlphaCode

\textbf{Engineering:} Testing, bug detection, refactoring

\textbf{Low-code:} NL to app, UI generation
\end{column}

\begin{column}{0.32\textwidth}
\centering
\textcolor{chartorange}{\textbf{Science \& Research}}

\textbf{Drug Discovery:} AlphaFold, molecule generation

\textbf{Writing:} Literature review, hypothesis generation

\textbf{Analysis:} Automated insights, visualization
\end{column}
\end{columns}
\normalsize

\bottomnote{Generative AI transforms creative industries - text, image, code, and scientific generation achieve practical utility across domains}
\end{frame}

% Ethical Considerations
\begin{frame}{Ethical Considerations in Generative AI}
\twocolslide{
\textbf{Key Challenges}
\small
\begin{itemize}
\item \textbf{Bias:} Training data propagates biases
\item \textbf{Misinformation:} Deepfakes, synthetic media
\item \textbf{Copyright:} Training on protected content
\item \textbf{Privacy:} Data memorization risks
\end{itemize}
\normalsize
}{
\textbf{Mitigation Strategies}
\small
\begin{itemize}
\item \textbf{Technical:} Bias detection, differential privacy
\item \textbf{Regulatory:} AI governance, content labeling
\item \textbf{Industry:} Responsible AI principles, auditing
\item \textbf{Education:} Media literacy, ethics training
\end{itemize}
\normalsize
}

\bottomnote{Ethical challenges accompany generative capabilities - deepfakes, copyright, bias, and labor displacement require comprehensive governance frameworks}
\end{frame}

% Future Directions
\begin{frame}{The Future of Generative AI}
\twocolslide{
\textbf{Technical Frontiers}
\small
\begin{itemize}
\item \textbf{Multimodal:} GPT-4V, video-language models
\item \textbf{Efficiency:} Compression, edge deployment
\item \textbf{Controllability:} Fine-grained, user-guided
\item \textbf{Reasoning:} Chain-of-thought, math, science
\end{itemize}
\normalsize
}{
\textbf{Societal Impact}
\small
\begin{itemize}
\item \textbf{Creative:} AI-human collaboration, new workflows
\item \textbf{Education:} Personalized learning, AI tutoring
\item \textbf{Business:} Automated pipelines, personalization
\end{itemize}
\normalsize

\keypoint{Key:} AI increasingly integrated into daily life
}

\bottomnote{Future generative AI balances technical advancement with societal responsibility - multimodal capabilities and efficiency improvements continue}
\end{frame}