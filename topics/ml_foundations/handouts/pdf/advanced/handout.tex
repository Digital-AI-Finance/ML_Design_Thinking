% Pandoc LaTeX template for course handouts
% Usage: pandoc input.md -o output.tex --template=handout_template.tex --listings
\documentclass[10pt,a4paper]{article}

% Geometry
\usepackage[margin=2.5cm]{geometry}

% Fonts and encoding
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}

% Math
\usepackage{amsmath,amssymb,amsfonts}

% Tables
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}

% Graphics
\usepackage{graphicx}

% Colors
\usepackage{xcolor}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{codebg}{RGB}{248,248,248}
\definecolor{codeframe}{RGB}{200,200,200}

% Code listings
\usepackage{listings}
\lstset{
  backgroundcolor=\color{codebg},
  basicstyle=\ttfamily\small,
  breakatwhitespace=false,
  breaklines=true,
  captionpos=b,
  commentstyle=\color{gray},
  frame=single,
  framerule=0.5pt,
  rulecolor=\color{codeframe},
  keepspaces=true,
  keywordstyle=\color{mlblue}\bfseries,
  language=Python,
  numbers=none,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  stringstyle=\color{mlpurple},
  tabsize=4,
  xleftmargin=0.5em,
  xrightmargin=0.5em,
  aboveskip=1em,
  belowskip=1em
}

% Hyperlinks
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=mlblue,
  urlcolor=mlblue,
  citecolor=mlblue
}

% Headers and footers
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\lhead{\textcolor{mlpurple}{\textbf{ML Foundations}}}
\rhead{\textcolor{gray}{Advanced}}
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt}

% Paragraph spacing
\setlength{\parindent}{0pt}
\setlength{\parskip}{0.5em}

% Section formatting
\usepackage{titlesec}
\titleformat{\section}
  {\Large\bfseries\color{mlpurple}}
  {\thesection}{1em}{}
\titleformat{\subsection}
  {\large\bfseries\color{mlblue}}
  {\thesubsection}{1em}{}
\titleformat{\subsubsection}
  {\normalsize\bfseries}
  {\thesubsubsection}{1em}{}

% Lists
\usepackage{enumitem}
\setlist[itemize]{topsep=0.5em, itemsep=0.25em}
\setlist[enumerate]{topsep=0.5em, itemsep=0.25em}

% Tight list support for pandoc
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% Title
\title{\Huge\bfseries\color{mlpurple} Week 00a Advanced Handout: ML Foundations - Mathematical Theory}
\author{Machine Learning for Smarter Innovation}
\date{}

\begin{document}

\maketitle
\thispagestyle{fancy}

\section{Week 00a Advanced Handout: ML Foundations - Mathematical
Theory}\label{week-00a-advanced-handout-ml-foundations---mathematical-theory}

\subsection{For Students With: Calculus, linear algebra,
probability}\label{for-students-with-calculus-linear-algebra-probability}

\subsection{Statistical Learning
Theory}\label{statistical-learning-theory}

\subsubsection{Empirical Risk
Minimization}\label{empirical-risk-minimization}

\textbf{Goal}: Minimize expected loss over data distribution

\[
R(f) = \mathbb{E}_{(x,y) \sim P}[L(f(x), y)]
\]

\textbf{Empirical Risk} (training error): \[
\hat{R}(f) = \frac{1}{n}\sum_{i=1}^n L(f(x_i), y_i)
\]

\subsubsection{Bias-Variance
Decomposition}\label{bias-variance-decomposition}

For squared loss: \[
\mathbb{E}[(y - \hat{f}(x))^2] = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}
\]

\begin{itemize}
\tightlist
\item
  \textbf{Bias}: Error from wrong assumptions (underfitting)
\item
  \textbf{Variance}: Error from sensitivity to training set
  (overfitting)
\item
  \textbf{Tradeoff}: Simple models (high bias, low variance), Complex
  models (low bias, high variance)
\end{itemize}

\subsubsection{VC Dimension and
Generalization}\label{vc-dimension-and-generalization}

\textbf{VC Dimension} (Vapnik-Chervonenkis): Measure of model capacity

\textbf{Generalization Bound}: \[
R(f) \leq \hat{R}(f) + \sqrt{\frac{d(\log(2n/d) + 1) - \log(\delta/4)}{n}}
\]

Where: - \(d\) = VC dimension - \(n\) = training samples - \(\delta\) =
confidence level

\textbf{Insight}: Generalization improves with more data, degrades with
model complexity

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{PAC Learning Framework}\label{pac-learning-framework}

\subsubsection{Probably Approximately Correct (PAC)
Learning}\label{probably-approximately-correct-pac-learning}

A concept class is PAC-learnable if:

Given: - Accuracy \(\epsilon > 0\) - Confidence \(\delta > 0\) - Sample
complexity \(m(\epsilon, \delta)\)

Algorithm outputs hypothesis \(h\) with: \[
P(R(h) \leq \epsilon) \geq 1 - \delta
\]

\subsubsection{Sample Complexity Bounds}\label{sample-complexity-bounds}

For finite hypothesis space \(|\mathcal{H}|\): \[
m \geq \frac{1}{\epsilon}\left(\log|\mathcal{H}| + \log\frac{1}{\delta}\right)
\]

\textbf{Implication}: Need more samples for complex models

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Optimization Theory}\label{optimization-theory}

\subsubsection{Gradient Descent
Convergence}\label{gradient-descent-convergence}

For convex \(L\)-Lipschitz functions with learning rate
\(\eta = \frac{1}{\sqrt{t}}\):

\[
R(w_t) - R(w^*) \leq \frac{L\|w_0 - w^*\|}{\sqrt{t}}
\]

Converges at rate \(O(1/\sqrt{t})\)

\subsubsection{Stochastic Gradient Descent
(SGD)}\label{stochastic-gradient-descent-sgd}

Update rule: \[
w_{t+1} = w_t - \eta_t \nabla L(w_t; x_i, y_i)
\]

\textbf{Advantages}: - Faster per-iteration (single sample) - Escapes
local minima (noise helps) - Online learning compatible

\textbf{Convergence}: \(O(1/\sqrt{t})\) with proper learning rate
schedule

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Regularization Theory}\label{regularization-theory}

\subsubsection{Ridge Regression (L2)}\label{ridge-regression-l2}

\[
\min_w \|Xw - y\|^2 + \lambda\|w\|^2
\]

\textbf{Closed Form}: \[
w^* = (X^TX + \lambda I)^{-1}X^Ty
\]

\textbf{Bayesian Interpretation}: Gaussian prior on weights

\subsubsection{Lasso (L1)}\label{lasso-l1}

\[
\min_w \|Xw - y\|^2 + \lambda\|w\|_1
\]

\textbf{Properties}: - Sparse solutions (many \(w_i = 0\)) - Feature
selection - No closed form (use proximal gradient)

\subsubsection{Elastic Net}\label{elastic-net}

\[
\min_w \|Xw - y\|^2 + \lambda_1\|w\|_1 + \lambda_2\|w\|^2
\]

Combines L1 sparsity with L2 stability

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Kernel Methods}\label{kernel-methods}

\subsubsection{Kernel Trick}\label{kernel-trick}

Implicit high-dimensional mapping via kernel function: \[
K(x, x') = \langle \phi(x), \phi(x') \rangle
\]

\textbf{Common Kernels}: 1. \textbf{Linear}: \(K(x, x') = x^T x'\) 2.
\textbf{Polynomial}: \(K(x, x') = (x^T x' + c)^d\) 3. \textbf{RBF}:
\(K(x, x') = \exp(-\gamma\|x - x'\|^2)\)

\subsubsection{Representer Theorem}\label{representer-theorem}

Optimal solution lives in span of training data: \[
f^*(x) = \sum_{i=1}^n \alpha_i K(x_i, x)
\]

\textbf{Implication}: Can solve in dual space (useful when \(d \gg n\))

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Information Theory in ML}\label{information-theory-in-ml}

\subsubsection{Cross-Entropy Loss}\label{cross-entropy-loss}

\[
H(p, q) = -\sum_i p_i \log q_i
\]

\textbf{Classification}: Minimizing cross-entropy = maximizing
likelihood

\subsubsection{KL Divergence}\label{kl-divergence}

\[
D_{KL}(P\|Q) = \sum_i p_i \log\frac{p_i}{q_i}
\]

\textbf{Properties}: - Non-negative - Zero iff \(P = Q\) - Asymmetric

\textbf{Use}: Measure distribution mismatch (VAEs, RL)

\subsubsection{Mutual Information}\label{mutual-information}

\[
I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
\]

\textbf{Application}: Feature selection (maximize \(I(X_i; Y)\))

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Concentration
Inequalities}\label{concentration-inequalities}

\subsubsection{Hoeffding's Inequality}\label{hoeffdings-inequality}

\[
P(|\hat{\mu} - \mu| \geq \epsilon) \leq 2\exp(-2n\epsilon^2)
\]

\textbf{Application}: Confidence intervals for empirical mean

\subsubsection{McDiarmid's Inequality}\label{mcdiarmids-inequality}

For bounded differences \(c_i\): \[
P(|f - \mathbb{E}[f]| \geq \epsilon) \leq 2\exp\left(-\frac{2\epsilon^2}{\sum c_i^2}\right)
\]

\textbf{Application}: Generalization bounds for more complex functions

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Advanced Topics}\label{advanced-topics}

\subsubsection{1. Online Learning}\label{online-learning}

\textbf{Regret Bound}: \[
\text{Regret}_T = \sum_{t=1}^T L(w_t, x_t, y_t) - \min_w \sum_{t=1}^T L(w, x_t, y_t)
\]

\textbf{Goal}: Sublinear regret \(o(T)\)

\subsubsection{2. Multi-Armed Bandits}\label{multi-armed-bandits}

\textbf{Explore-Exploit Tradeoff}: Upper Confidence Bound (UCB) \[
a_t = \argmax_a \left(\hat{\mu}_a + \sqrt{\frac{2\log t}{n_a}}\right)
\]

\subsubsection{3. Boosting Theory}\label{boosting-theory}

\textbf{AdaBoost} minimizes exponential loss: \[
L(\alpha, w) = \sum_i \exp(-y_i f(x_i))
\]

\textbf{Margin Theory}: Boosting increases minimum margin

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Proofs}\label{proofs}

\subsubsection{Proof: Ridge Regression
Solution}\label{proof-ridge-regression-solution}

Minimize: \[
L(w) = \|Xw - y\|^2 + \lambda\|w\|^2
\]

Take gradient: \[
\nabla_w L = 2X^T(Xw - y) + 2\lambda w = 0
\]

Solve for \(w\): \[
X^TXw + \lambda w = X^Ty
\] \[
(X^TX + \lambda I)w = X^Ty
\] \[
w^* = (X^TX + \lambda I)^{-1}X^Ty
\]

\textbf{Note}: \(\lambda I\) ensures invertibility even when \(X^TX\)
singular

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Practice Problems}\label{practice-problems}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Derive} logistic regression gradient
\item
  \textbf{Prove} bias-variance decomposition for squared loss
\item
  \textbf{Show} SVMs solve dual problem using KKT conditions
\item
  \textbf{Compute} VC dimension of linear classifiers in
  \(\mathbb{R}^d\)
\item
  \textbf{Analyze} convergence rate of batch vs stochastic gradient
  descent
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{References}\label{references}

\begin{itemize}
\tightlist
\item
  Shalev-Shwartz \& Ben-David: Understanding Machine Learning
\item
  Hastie, Tibshirani, Friedman: Elements of Statistical Learning
\item
  Vapnik: Statistical Learning Theory
\item
  Bishop: Pattern Recognition and Machine Learning
\end{itemize}

\end{document}
