% Pandoc LaTeX template for course handouts
% Usage: pandoc input.md -o output.tex --template=handout_template.tex --listings
\documentclass[10pt,a4paper]{article}

% Geometry
\usepackage[margin=2.5cm]{geometry}

% Fonts and encoding
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}

% Math
\usepackage{amsmath,amssymb,amsfonts}

% Tables
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}

% Graphics
\usepackage{graphicx}

% Colors
\usepackage{xcolor}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{codebg}{RGB}{248,248,248}
\definecolor{codeframe}{RGB}{200,200,200}

% Code listings
\usepackage{listings}
\lstset{
  backgroundcolor=\color{codebg},
  basicstyle=\ttfamily\small,
  breakatwhitespace=false,
  breaklines=true,
  captionpos=b,
  commentstyle=\color{gray},
  frame=single,
  framerule=0.5pt,
  rulecolor=\color{codeframe},
  keepspaces=true,
  keywordstyle=\color{mlblue}\bfseries,
  language=Python,
  numbers=none,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  stringstyle=\color{mlpurple},
  tabsize=4,
  xleftmargin=0.5em,
  xrightmargin=0.5em,
  aboveskip=1em,
  belowskip=1em
}

% Hyperlinks
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=mlblue,
  urlcolor=mlblue,
  citecolor=mlblue
}

% Headers and footers
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\lhead{\textcolor{mlpurple}{\textbf{Neural Networks}}}
\rhead{\textcolor{gray}{Advanced}}
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt}

% Paragraph spacing
\setlength{\parindent}{0pt}
\setlength{\parskip}{0.5em}

% Section formatting
\usepackage{titlesec}
\titleformat{\section}
  {\Large\bfseries\color{mlpurple}}
  {\thesection}{1em}{}
\titleformat{\subsection}
  {\large\bfseries\color{mlblue}}
  {\thesubsection}{1em}{}
\titleformat{\subsubsection}
  {\normalsize\bfseries}
  {\thesubsubsection}{1em}{}

% Lists
\usepackage{enumitem}
\setlist[itemize]{topsep=0.5em, itemsep=0.25em}
\setlist[enumerate]{topsep=0.5em, itemsep=0.25em}

% Tight list support for pandoc
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% Title
\title{\Huge\bfseries\color{mlpurple} Neural Networks - Advanced Handout}
\author{Machine Learning for Smarter Innovation}
\date{}

\begin{document}

\maketitle
\thispagestyle{fancy}

\section{Neural Networks - Advanced
Handout}\label{neural-networks---advanced-handout}

\textbf{Target Audience}: Data scientists and ML engineers
\textbf{Duration}: 90 minutes reading \textbf{Level}: Advanced
(mathematical foundations, optimization theory)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Mathematical Foundations}\label{mathematical-foundations}

\subsubsection{Forward Propagation}\label{forward-propagation}

For layer \(l\) with weights \(W^{(l)}\), bias \(b^{(l)}\), and
activation \(f\):

\[z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\] \[a^{(l)} = f(z^{(l)})\]

Where: - \(a^{(0)} = x\) (input) - \(a^{(L)} = \hat{y}\) (output)

\subsubsection{Backpropagation}\label{backpropagation}

\textbf{Loss gradient with respect to output}:
\[\delta^{(L)} = \nabla_{a^{(L)}} \mathcal{L} \odot f'(z^{(L)})\]

\textbf{Gradient propagation} (for \(l = L-1, ..., 1\)):
\[\delta^{(l)} = (W^{(l+1)})^T \delta^{(l+1)} \odot f'(z^{(l)})\]

\textbf{Weight gradients}:
\[\frac{\partial \mathcal{L}}{\partial W^{(l)}} = \delta^{(l)} (a^{(l-1)})^T\]
\[\frac{\partial \mathcal{L}}{\partial b^{(l)}} = \delta^{(l)}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Activation Functions}\label{activation-functions}

\subsubsection{ReLU and Variants}\label{relu-and-variants}

\textbf{ReLU}: \(f(x) = \max(0, x)\), \(f'(x) = \mathbf{1}_{x > 0}\)

\textbf{Leaky ReLU}: \(f(x) = \max(\alpha x, x)\) where
\(\alpha \approx 0.01\)

\textbf{GELU} (Gaussian Error Linear Unit):
\[f(x) = x \cdot \Phi(x) \approx 0.5x(1 + \tanh[\sqrt{2/\pi}(x + 0.044715x^3)])\]

\textbf{Swish}: \(f(x) = x \cdot \sigma(\beta x)\)

\subsubsection{Softmax for
Classification}\label{softmax-for-classification}

\[\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}\]

\textbf{Numerical stability}:
\[\text{softmax}(z_i) = \frac{e^{z_i - \max(z)}}{\sum_{j=1}^K e^{z_j - \max(z)}}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Loss Functions}\label{loss-functions}

\subsubsection{Cross-Entropy Loss}\label{cross-entropy-loss}

\textbf{Binary}:
\[\mathcal{L} = -\frac{1}{N}\sum_{i=1}^N [y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]\]

\textbf{Multi-class}:
\[\mathcal{L} = -\frac{1}{N}\sum_{i=1}^N \sum_{c=1}^C y_{ic} \log(\hat{y}_{ic})\]

\subsubsection{Focal Loss (for imbalanced
data)}\label{focal-loss-for-imbalanced-data}

\[\mathcal{L}_{FL} = -\alpha_t (1-p_t)^\gamma \log(p_t)\]

Where \(\gamma\) is focusing parameter (typically 2).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Optimization Algorithms}\label{optimization-algorithms}

\subsubsection{Stochastic Gradient Descent
(SGD)}\label{stochastic-gradient-descent-sgd}

\[\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}(\theta_t; x_i, y_i)\]

\subsubsection{SGD with Momentum}\label{sgd-with-momentum}

\[v_t = \gamma v_{t-1} + \eta \nabla_\theta \mathcal{L}\]
\[\theta_{t+1} = \theta_t - v_t\]

\subsubsection{Adam (Adaptive Moment
Estimation)}\label{adam-adaptive-moment-estimation}

\[m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t\]
\[v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2\]
\[\hat{m}_t = \frac{m_t}{1-\beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1-\beta_2^t}\]
\[\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t\]

Default: \(\beta_1 = 0.9\), \(\beta_2 = 0.999\), \(\epsilon = 10^{-8}\)

\subsubsection{AdamW (Weight Decay
Decoupled)}\label{adamw-weight-decay-decoupled}

\[\theta_{t+1} = \theta_t - \eta \left( \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \theta_t \right)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Initialization Strategies}\label{initialization-strategies}

\subsubsection{Xavier/Glorot
Initialization}\label{xavierglorot-initialization}

For layer with \(n_{in}\) inputs and \(n_{out}\) outputs:

\[W \sim \mathcal{U}\left(-\sqrt{\frac{6}{n_{in} + n_{out}}}, \sqrt{\frac{6}{n_{in} + n_{out}}}\right)\]

\textbf{For tanh activation}. Maintains variance through layers.

\subsubsection{He/Kaiming
Initialization}\label{hekaiming-initialization}

\[W \sim \mathcal{N}\left(0, \sqrt{\frac{2}{n_{in}}}\right)\]

\textbf{For ReLU activation}. Accounts for ReLU zeroing half the
activations.

\begin{lstlisting}[language=Python]
# PyTorch implementation
nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')
nn.init.xavier_uniform_(layer.weight)
\end{lstlisting}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Batch Normalization}\label{batch-normalization}

\subsubsection{Forward Pass}\label{forward-pass}

\[\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}\]
\[y_i = \gamma \hat{x}_i + \beta\]

Where \(\mu_B\), \(\sigma_B^2\) are batch statistics; \(\gamma\),
\(\beta\) are learned.

\subsubsection{Inference}\label{inference}

Use running averages instead of batch statistics:
\[\mu_{running} = (1-\alpha)\mu_{running} + \alpha \mu_B\]

\subsubsection{Layer Normalization (for
sequences)}\label{layer-normalization-for-sequences}

Normalize across features instead of batch:
\[\hat{x}_i = \frac{x_i - \mu_L}{\sqrt{\sigma_L^2 + \epsilon}}\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Regularization Theory}\label{regularization-theory}

\subsubsection{Dropout}\label{dropout}

During training, randomly zero activations with probability \(p\):
\[\tilde{a} = \frac{1}{1-p} \cdot a \cdot m, \quad m_i \sim \text{Bernoulli}(1-p)\]

\textbf{Interpretation}: Ensemble of \(2^n\) sub-networks.

\subsubsection{L2 Regularization (Weight
Decay)}\label{l2-regularization-weight-decay}

\[\mathcal{L}_{total} = \mathcal{L}_{data} + \frac{\lambda}{2}\|W\|_2^2\]

Gradient becomes:
\[\nabla_W \mathcal{L}_{total} = \nabla_W \mathcal{L}_{data} + \lambda W\]

\subsubsection{Label Smoothing}\label{label-smoothing}

Instead of one-hot targets, use:
\[y_{smooth} = (1-\alpha)y_{one-hot} + \frac{\alpha}{K}\]

Typically \(\alpha = 0.1\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Attention Mechanism}\label{attention-mechanism}

\subsubsection{Scaled Dot-Product
Attention}\label{scaled-dot-product-attention}

\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]

Where: - \(Q \in \mathbb{R}^{n \times d_k}\) (queries) -
\(K \in \mathbb{R}^{m \times d_k}\) (keys) -
\(V \in \mathbb{R}^{m \times d_v}\) (values)

\subsubsection{Multi-Head Attention}\label{multi-head-attention}

\[\text{MultiHead}(Q, K, V) = \text{Concat}(head_1, ..., head_h)W^O\]
\[head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\]

\subsubsection{Self-Attention}\label{self-attention}

When \(Q = K = V = X\) (same input):
\[\text{SelfAttention}(X) = \text{softmax}\left(\frac{XX^T}{\sqrt{d}}\right)X\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Transformer Architecture}\label{transformer-architecture}

\subsubsection{Encoder Block}\label{encoder-block}

\begin{lstlisting}
Input -> LayerNorm -> MultiHeadAttention -> Residual -> LayerNorm -> FFN -> Residual -> Output
\end{lstlisting}

\subsubsection{Position Encoding}\label{position-encoding}

\[PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})\]
\[PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})\]

\subsubsection{Implementation}\label{implementation}

\begin{lstlisting}[language=Python]
class TransformerBlock(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.ff = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout)
        )

    def forward(self, x, mask=None):
        # Self-attention with residual
        attn_out, _ = self.attention(x, x, x, attn_mask=mask)
        x = self.norm1(x + attn_out)

        # Feed-forward with residual
        ff_out = self.ff(x)
        x = self.norm2(x + ff_out)

        return x
\end{lstlisting}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Gradient Issues}\label{gradient-issues}

\subsubsection{Vanishing Gradients}\label{vanishing-gradients}

\textbf{Cause}: Saturating activations (sigmoid, tanh) or deep networks.

\textbf{Solutions}: - ReLU activations - Batch/Layer normalization -
Residual connections - LSTM/GRU for sequences

\subsubsection{Exploding Gradients}\label{exploding-gradients}

\textbf{Cause}: Large weight magnitudes, especially in RNNs.

\textbf{Solutions}: - Gradient clipping:
\(g \leftarrow \min(1, \frac{\theta}{\|g\|}) \cdot g\) - Weight
initialization - Layer normalization

\begin{lstlisting}[language=Python]
# Gradient clipping in PyTorch
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
\end{lstlisting}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Learning Rate Scheduling}\label{learning-rate-scheduling}

\subsubsection{Cosine Annealing}\label{cosine-annealing}

\[\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})(1 + \cos(\frac{t}{T}\pi))\]

\subsubsection{Warmup + Decay}\label{warmup-decay}

\[\eta_t = \begin{cases} \eta_{max} \cdot \frac{t}{T_{warmup}} & t < T_{warmup} \\ \eta_{max} \cdot \text{decay}(t - T_{warmup}) & t \geq T_{warmup} \end{cases}\]

\begin{lstlisting}[language=Python]
# PyTorch scheduler
scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
    optimizer, T_0=10, T_mult=2
)

# OneCycleLR (recommended for training)
scheduler = optim.lr_scheduler.OneCycleLR(
    optimizer, max_lr=0.01, epochs=epochs, steps_per_epoch=len(train_loader)
)
\end{lstlisting}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Mixed Precision Training}\label{mixed-precision-training}

\subsubsection{FP16 Training}\label{fp16-training}

\begin{lstlisting}[language=Python]
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for batch in train_loader:
    optimizer.zero_grad()

    with autocast():
        outputs = model(inputs)
        loss = criterion(outputs, targets)

    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
\end{lstlisting}

\textbf{Benefits}: 2x memory reduction, faster training on modern GPUs.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Distributed Training}\label{distributed-training}

\subsubsection{Data Parallel}\label{data-parallel}

\begin{lstlisting}[language=Python]
model = nn.DataParallel(model)  # Simple, single-machine multi-GPU
\end{lstlisting}

\subsubsection{Distributed Data
Parallel}\label{distributed-data-parallel}

\begin{lstlisting}[language=Python]
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

dist.init_process_group("nccl")
model = DDP(model, device_ids=[local_rank])
\end{lstlisting}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Model Compression}\label{model-compression}

\subsubsection{Quantization}\label{quantization}

\begin{lstlisting}[language=Python]
# Post-training quantization
quantized_model = torch.quantization.quantize_dynamic(
    model, {nn.Linear}, dtype=torch.qint8
)
\end{lstlisting}

\subsubsection{Knowledge Distillation}\label{knowledge-distillation}

\[\mathcal{L}_{KD} = \alpha \mathcal{L}_{CE}(y, \hat{y}_{student}) + (1-\alpha) T^2 \mathcal{L}_{KL}(\sigma(z_T/T), \sigma(z_S/T))\]

\subsubsection{Pruning}\label{pruning}

\begin{lstlisting}[language=Python]
import torch.nn.utils.prune as prune

# Prune 30% of weights with smallest magnitude
prune.l1_unstructured(module, name='weight', amount=0.3)
\end{lstlisting}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{References}\label{references}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Goodfellow, I., Bengio, Y., \& Courville, A. (2016). Deep Learning.
  MIT Press.
\item
  Vaswani, A., et al.~(2017). ``Attention Is All You Need''
\item
  He, K., et al.~(2016). ``Deep Residual Learning for Image
  Recognition''
\item
  Kingma, D. P., \& Ba, J. (2015). ``Adam: A Method for Stochastic
  Optimization''
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\emph{Deep learning is an empirical science. Theory provides guidance,
but experimentation determines success.}

\end{document}
