{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Week 1: Clustering for Innovation - Part 2\n## Technical Implementation & Design Integration\n\nThis notebook continues from Part 1, covering the technical deep dive and design applications.\n\n**Part 2 Contents:**\n- Section 0: Complete Setup & ALL Functions (50+ total)\n- Section 3: Technical Deep Dive (function calls only)\n- Section 4: Design Integration (function calls only)\n\n**Note:** All code is organized as functions at the beginning for modularity and reusability.\n**Prerequisites:** Run Part 1 first to understand the foundation, or use the quick setup below."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Setup\n",
    "If you're starting directly with Part 2, run this cell to import essential functions from Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports (if starting fresh)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Part 2 setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Design Integration Functions\n\ndef transform_clusters_to_insights():\n    \"\"\"\n    Transform technical clustering results into actionable innovation insights.\n    Generate comprehensive innovation dataset and apply clustering.\n    \"\"\"\n    print(\"üí° Transforming Clusters into Innovation Insights\\n\")\n    \n    # Generate comprehensive innovation dataset\n    n_innovations = 1000\n    n_features = 10\n    n_clusters = 5\n    \n    # Generate base data\n    from sklearn.datasets import make_blobs\n    X_innovation, y_true = make_blobs(n_samples=n_innovations, \n                                     n_features=n_features,\n                                     centers=n_clusters,\n                                     cluster_std=1.2,\n                                     random_state=42)\n    \n    # Feature names\n    feature_names = [\n        'Technical_Complexity', 'Market_Readiness', 'Investment_Required',\n        'User_Impact', 'Implementation_Time', 'Risk_Level',\n        'Innovation_Score', 'Scalability', 'Regulatory_Compliance', 'ROI_Potential'\n    ]\n    \n    # Standardize\n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X_innovation)\n    \n    # Apply clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    labels = kmeans.fit_predict(X_scaled)\n    \n    # Create DataFrame\n    innovation_df = pd.DataFrame(X_innovation, columns=feature_names)\n    innovation_df['Cluster'] = labels\n    \n    # Visualize clusters in 2D using PCA\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X_scaled)\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n    \n    # Scatter plot\n    colors = plt.cm.Set3(np.linspace(0, 1, n_clusters))\n    for i in range(n_clusters):\n        mask = labels == i\n        ax1.scatter(X_pca[mask, 0], X_pca[mask, 1], \n                   c=[colors[i]], s=30, alpha=0.6,\n                   label=f'Cluster {i+1}', edgecolors='black', linewidth=0.5)\n    \n    # Add cluster centers\n    centers_pca = pca.transform(kmeans.cluster_centers_)\n    ax1.scatter(centers_pca[:, 0], centers_pca[:, 1],\n               c='black', marker='*', s=300,\n               edgecolors='white', linewidth=2, zorder=10)\n    \n    ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=11)\n    ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=11)\n    ax1.set_title('Innovation Clusters (PCA Visualization)', fontsize=12, fontweight='bold')\n    ax1.legend(loc='best')\n    ax1.grid(True, alpha=0.3)\n    \n    # Cluster sizes\n    cluster_sizes = innovation_df['Cluster'].value_counts().sort_index()\n    ax2.bar(range(n_clusters), cluster_sizes.values, color=colors, alpha=0.7, edgecolor='black')\n    ax2.set_xlabel('Cluster', fontsize=11)\n    ax2.set_ylabel('Number of Innovations', fontsize=11)\n    ax2.set_title('Innovation Distribution Across Clusters', fontsize=12, fontweight='bold')\n    ax2.set_xticks(range(n_clusters))\n    ax2.set_xticklabels([f'Cluster {i+1}' for i in range(n_clusters)])\n    \n    # Add value labels\n    for i, v in enumerate(cluster_sizes.values):\n        ax2.text(i, v + 5, str(v), ha='center', fontweight='bold')\n    \n    plt.suptitle('Innovation Landscape Overview', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\nüìä Innovation Clustering Results:\")\n    print(f\"Total Innovations: {n_innovations}\")\n    print(f\"Number of Clusters: {n_clusters}\")\n    print(f\"Average Cluster Size: {n_innovations/n_clusters:.0f}\")\n    print(f\"Silhouette Score: {silhouette_score(X_scaled, labels):.3f}\")\n    \n    return innovation_df, X_scaled, labels, kmeans\n\n\ndef create_innovation_archetypes():\n    \"\"\"\n    Create innovation archetypes from clusters with detailed characterization.\n    Maps clusters to meaningful innovation personas.\n    \"\"\"\n    print(\"üé≠ Creating Innovation Archetypes\\n\")\n    \n    # Get data from previous function or generate new\n    innovation_df, X_scaled, labels, kmeans = transform_clusters_to_insights()\n    \n    n_clusters = len(np.unique(labels))\n    \n    # Define archetype characteristics\n    archetype_names = [\n        'Digital Pioneers',\n        'Market Disruptors', \n        'Efficiency Optimizers',\n        'Customer Champions',\n        'Platform Builders'\n    ]\n    \n    archetype_descriptions = [\n        'High-tech, high-risk innovations targeting early adopters',\n        'Game-changing solutions that redefine market dynamics',\n        'Process improvements focusing on cost and time savings',\n        'User-centric innovations prioritizing experience',\n        'Ecosystem solutions creating network effects'\n    ]\n    \n    # Analyze each cluster\n    archetypes = []\n    feature_names = innovation_df.columns[:-1]  # Exclude 'Cluster' column\n    \n    for cluster_id in range(n_clusters):\n        cluster_data = innovation_df[innovation_df['Cluster'] == cluster_id]\n        \n        # Calculate statistics\n        archetype = {\n            'Cluster': cluster_id + 1,\n            'Name': archetype_names[cluster_id % len(archetype_names)],\n            'Description': archetype_descriptions[cluster_id % len(archetype_descriptions)],\n            'Size': len(cluster_data),\n            'Percentage': f\"{len(cluster_data)/len(innovation_df)*100:.1f}%\"\n        }\n        \n        # Top features\n        feature_means = cluster_data[feature_names].mean()\n        top_features = feature_means.nlargest(3).index.tolist()\n        archetype['Top_Features'] = ', '.join(top_features)\n        \n        # Risk profile\n        if 'Risk_Level' in cluster_data.columns:\n            risk_level = cluster_data['Risk_Level'].mean()\n            if risk_level > 0.5:\n                archetype['Risk_Profile'] = 'High Risk'\n            elif risk_level > -0.5:\n                archetype['Risk_Profile'] = 'Medium Risk'\n            else:\n                archetype['Risk_Profile'] = 'Low Risk'\n        \n        archetypes.append(archetype)\n    \n    # Create archetype cards visualization\n    fig, axes = plt.subplots(1, n_clusters, figsize=(18, 6))\n    if n_clusters == 1:\n        axes = [axes]\n    \n    colors = plt.cm.Set3(np.linspace(0, 1, n_clusters))\n    \n    for idx, archetype in enumerate(archetypes):\n        ax = plt.subplot(1, n_clusters, idx+1, projection='polar')\n        \n        # Create radar chart for each archetype\n        cluster_data = innovation_df[innovation_df['Cluster'] == idx]\n        feature_values = cluster_data[feature_names[:6]].mean().values\n        \n        # Normalize to 0-1 scale\n        feature_values = (feature_values - feature_values.min()) / (feature_values.max() - feature_values.min() + 1e-10)\n        \n        # Create radar chart\n        angles = np.linspace(0, 2*np.pi, len(feature_names[:6]), endpoint=False)\n        feature_values = np.concatenate((feature_values, [feature_values[0]]))\n        angles = np.concatenate((angles, [angles[0]]))\n        \n        ax.plot(angles, feature_values, 'o-', linewidth=2, color=colors[idx])\n        ax.fill(angles, feature_values, alpha=0.25, color=colors[idx])\n        ax.set_xticks(angles[:-1])\n        ax.set_xticklabels([f.replace('_', '\\n') for f in feature_names[:6]], fontsize=8)\n        ax.set_ylim(0, 1)\n        ax.set_title(f\"{archetype['Name']}\\n({archetype['Size']} innovations)\", \n                    fontsize=10, fontweight='bold', pad=20)\n        ax.grid(True)\n    \n    plt.suptitle('Innovation Archetype Profiles', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    # Display archetype summary\n    archetypes_df = pd.DataFrame(archetypes)\n    print(\"\\nüìã Innovation Archetype Summary:\")\n    display(archetypes_df[['Name', 'Size', 'Percentage', 'Risk_Profile', 'Top_Features']])\n    \n    print(\"\\nüí° How to Use Archetypes:\")\n    print(\"‚Ä¢ Tailor innovation strategies per archetype\")\n    print(\"‚Ä¢ Allocate resources based on archetype characteristics\")\n    print(\"‚Ä¢ Design specific support programs for each type\")\n    print(\"‚Ä¢ Track archetype evolution over time\")\n    \n    return archetypes_df, innovation_df\n\n\ndef generate_opportunity_analysis():\n    \"\"\"\n    Generate comprehensive opportunity analysis with heatmaps and priority matrices.\n    Identifies white spaces and strategic opportunities.\n    \"\"\"\n    print(\"üî• Innovation Opportunity Analysis\\n\")\n    \n    # Get clustered data\n    innovation_df, X_scaled, labels, kmeans = transform_clusters_to_insights()\n    n_clusters = len(np.unique(labels))\n    \n    # Calculate opportunity scores\n    opportunity_dimensions = [\n        'Market_Size', 'Growth_Rate', 'Competition',\n        'Tech_Readiness', 'Investment_Need', 'Time_to_Market',\n        'Risk_Level', 'Regulatory', 'Customer_Demand'\n    ]\n    \n    # Create opportunity matrix\n    np.random.seed(42)\n    opportunity_matrix = np.random.randn(n_clusters, len(opportunity_dimensions))\n    \n    # Create heatmap\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n    \n    # Heatmap\n    im = ax1.imshow(opportunity_matrix, cmap='RdYlGn', aspect='auto', vmin=-2, vmax=2)\n    ax1.set_xticks(range(len(opportunity_dimensions)))\n    ax1.set_xticklabels(opportunity_dimensions, rotation=45, ha='right')\n    ax1.set_yticks(range(n_clusters))\n    ax1.set_yticklabels([f'Cluster {i+1}' for i in range(n_clusters)])\n    ax1.set_title('Innovation Opportunity Heatmap', fontsize=12, fontweight='bold')\n    \n    # Add values\n    for i in range(n_clusters):\n        for j in range(len(opportunity_dimensions)):\n            text = ax1.text(j, i, f'{opportunity_matrix[i, j]:.1f}',\n                           ha='center', va='center', color='black', fontsize=8)\n    \n    plt.colorbar(im, ax=ax1, label='Opportunity Score')\n    \n    # Priority matrix\n    cluster_sizes = innovation_df['Cluster'].value_counts().sort_index()\n    impact = innovation_df.groupby('Cluster')['User_Impact'].mean().values\n    effort = innovation_df.groupby('Cluster')['Implementation_Time'].mean().values\n    \n    colors = plt.cm.Set3(np.linspace(0, 1, n_clusters))\n    \n    ax2.scatter(effort, impact, s=cluster_sizes.values*2, c=colors, \n               alpha=0.6, edgecolors='black', linewidth=2)\n    \n    for i in range(n_clusters):\n        ax2.annotate(f'C{i+1}', (effort[i], impact[i]),\n                    ha='center', va='center', fontsize=9, fontweight='bold')\n    \n    # Add quadrant lines\n    ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n    ax2.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n    \n    # Add quadrant labels\n    ax2.text(1, 1, 'High Impact\\nHigh Effort', ha='center', va='center', \n            fontsize=10, alpha=0.5, bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.3))\n    ax2.text(-1, 1, 'High Impact\\nLow Effort', ha='center', va='center', \n            fontsize=10, alpha=0.5, bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.3))\n    ax2.text(-1, -1, 'Low Impact\\nLow Effort', ha='center', va='center', \n            fontsize=10, alpha=0.5, bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n    ax2.text(1, -1, 'Low Impact\\nHigh Effort', ha='center', va='center', \n            fontsize=10, alpha=0.5, bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.3))\n    \n    ax2.set_xlabel('Implementation Effort', fontsize=11)\n    ax2.set_ylabel('User Impact', fontsize=11)\n    ax2.set_title('Innovation Priority Matrix', fontsize=12, fontweight='bold')\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nüéØ Strategic Recommendations:\")\n    print(\"\\nüü¢ Quick Wins (High Impact, Low Effort):\")\n    print(\"  ‚Ä¢ Focus immediate resources here\")\n    print(\"  ‚Ä¢ Rapid prototyping and testing\")\n    print(\"\\nüü° Strategic Initiatives (High Impact, High Effort):\")\n    print(\"  ‚Ä¢ Long-term investment required\")\n    print(\"  ‚Ä¢ Build dedicated teams\")\n    print(\"\\nüîµ Fill-ins (Low Impact, Low Effort):\")\n    print(\"  ‚Ä¢ Good for learning and experimentation\")\n    print(\"  ‚Ä¢ Assign to junior teams\")\n    print(\"\\nüî¥ Avoid (Low Impact, High Effort):\")\n    print(\"  ‚Ä¢ Deprioritize or eliminate\")\n    print(\"  ‚Ä¢ Redirect resources elsewhere\")\n    \n    return opportunity_matrix, innovation_df\n\n\ndef build_innovation_taxonomy():\n    \"\"\"\n    Build hierarchical innovation taxonomy using hierarchical clustering.\n    Shows relationships between innovation clusters.\n    \"\"\"\n    print(\"üå≥ Building Innovation Taxonomy\\n\")\n    \n    # Get cluster centers from k-means\n    _, X_scaled, labels, kmeans = transform_clusters_to_insights()\n    \n    # Use cluster centers for hierarchical clustering\n    from scipy.cluster.hierarchy import dendrogram, linkage\n    \n    archetype_names = [\n        'Digital Pioneers',\n        'Market Disruptors',\n        'Efficiency Optimizers',\n        'Customer Champions',\n        'Platform Builders'\n    ]\n    \n    # Hierarchical clustering on centers\n    linkage_matrix = linkage(kmeans.cluster_centers_, method='ward')\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n    \n    # Dendrogram\n    dendrogram(linkage_matrix, ax=ax1, labels=archetype_names,\n              color_threshold=0, above_threshold_color='gray')\n    ax1.set_title('Innovation Taxonomy Hierarchy', fontsize=12, fontweight='bold')\n    ax1.set_xlabel('Innovation Archetype')\n    ax1.set_ylabel('Distance')\n    \n    # Lifecycle stages\n    lifecycle_stages = ['Ideation', 'Validation', 'Development', 'Launch', 'Scale', 'Maturity']\n    n_clusters = len(kmeans.cluster_centers_)\n    stage_distribution = np.random.dirichlet(np.ones(len(lifecycle_stages)), size=n_clusters)\n    \n    # Stack bar chart for lifecycle\n    bottom = np.zeros(n_clusters)\n    stage_colors = plt.cm.coolwarm(np.linspace(0, 1, len(lifecycle_stages)))\n    \n    for stage_idx, stage in enumerate(lifecycle_stages):\n        values = stage_distribution[:, stage_idx]\n        ax2.bar(range(n_clusters), values, bottom=bottom, \n               color=stage_colors[stage_idx], label=stage, alpha=0.8)\n        bottom += values\n    \n    ax2.set_xlabel('Innovation Archetype', fontsize=11)\n    ax2.set_ylabel('Proportion', fontsize=11)\n    ax2.set_title('Innovation Lifecycle Distribution by Archetype', fontsize=12, fontweight='bold')\n    ax2.set_xticks(range(n_clusters))\n    ax2.set_xticklabels([name.split()[0] for name in archetype_names], rotation=45)\n    ax2.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nüìä Taxonomy Insights:\")\n    print(\"‚Ä¢ Digital Pioneers and Market Disruptors are closely related\")\n    print(\"‚Ä¢ Efficiency Optimizers form a distinct branch\")\n    print(\"‚Ä¢ Platform Builders bridge multiple categories\")\n    \n    return linkage_matrix, stage_distribution\n\n\ndef create_innovation_ecosystem():\n    \"\"\"\n    Create innovation ecosystem network showing relationships between\n    archetypes and stakeholders.\n    \"\"\"\n    print(\"üåê Innovation Ecosystem Network\\n\")\n    \n    import networkx as nx\n    \n    # Create network graph\n    G = nx.Graph()\n    \n    archetype_names = [\n        'Digital Pioneers',\n        'Market Disruptors',\n        'Efficiency Optimizers',\n        'Customer Champions',\n        'Platform Builders'\n    ]\n    \n    # Add nodes for archetypes\n    cluster_sizes = [200, 180, 150, 170, 200]  # Example sizes\n    for i, name in enumerate(archetype_names):\n        G.add_node(name, node_type='archetype', size=cluster_sizes[i])\n    \n    # Add stakeholder nodes\n    stakeholders = ['Customers', 'Partners', 'Investors', 'Regulators', 'Competitors']\n    for stakeholder in stakeholders:\n        G.add_node(stakeholder, node_type='stakeholder', size=100)\n    \n    # Add edges (connections)\n    connections = [\n        ('Digital Pioneers', 'Investors', 0.8),\n        ('Digital Pioneers', 'Partners', 0.6),\n        ('Market Disruptors', 'Competitors', 0.9),\n        ('Market Disruptors', 'Customers', 0.7),\n        ('Efficiency Optimizers', 'Partners', 0.8),\n        ('Efficiency Optimizers', 'Regulators', 0.5),\n        ('Customer Champions', 'Customers', 0.9),\n        ('Customer Champions', 'Partners', 0.6),\n        ('Platform Builders', 'Partners', 0.9),\n        ('Platform Builders', 'Investors', 0.7)\n    ]\n    \n    for source, target, weight in connections:\n        G.add_edge(source, target, weight=weight)\n    \n    # Visualize network\n    fig, ax = plt.subplots(figsize=(12, 8))\n    \n    # Layout\n    pos = nx.spring_layout(G, k=2, iterations=50)\n    \n    # Draw nodes\n    archetype_nodes = [n for n in G.nodes() if G.nodes[n]['node_type'] == 'archetype']\n    stakeholder_nodes = [n for n in G.nodes() if G.nodes[n]['node_type'] == 'stakeholder']\n    \n    colors = plt.cm.Set3(np.linspace(0, 1, len(archetype_nodes)))\n    \n    # Archetype nodes\n    nx.draw_networkx_nodes(G, pos, nodelist=archetype_nodes,\n                          node_color=colors,\n                          node_size=[G.nodes[n]['size']*5 for n in archetype_nodes],\n                          alpha=0.7, ax=ax)\n    \n    # Stakeholder nodes\n    nx.draw_networkx_nodes(G, pos, nodelist=stakeholder_nodes,\n                          node_color='lightgray',\n                          node_size=500,\n                          node_shape='s',\n                          alpha=0.8, ax=ax)\n    \n    # Draw edges\n    edges = G.edges()\n    weights = [G[u][v]['weight'] for u, v in edges]\n    nx.draw_networkx_edges(G, pos, width=[w*3 for w in weights],\n                          alpha=0.5, ax=ax)\n    \n    # Labels\n    labels = {n: n.split()[0] if len(n.split()) > 1 else n for n in G.nodes()}\n    nx.draw_networkx_labels(G, pos, labels, font_size=10, font_weight='bold', ax=ax)\n    \n    ax.set_title('Innovation Ecosystem Network', fontsize=14, fontweight='bold')\n    ax.axis('off')\n    \n    # Add legend\n    from matplotlib.patches import Rectangle, Circle\n    legend_elements = [\n        Circle((0, 0), 0.1, facecolor=colors[0], alpha=0.7, label='Innovation Archetypes'),\n        Rectangle((0, 0), 0.1, 0.1, facecolor='lightgray', alpha=0.8, label='Stakeholders')\n    ]\n    ax.legend(handles=legend_elements, loc='upper right')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nüåê Ecosystem Insights:\")\n    print(\"‚Ä¢ Digital Pioneers have strong investor connections\")\n    print(\"‚Ä¢ Customer Champions directly connect with users\")\n    print(\"‚Ä¢ Platform Builders bridge multiple stakeholder groups\")\n    print(\"‚Ä¢ Market Disruptors create competitive tension\")\n    print(\"\\nüí° Use this network to:\")\n    print(\"‚Ä¢ Identify collaboration opportunities\")\n    print(\"‚Ä¢ Understand influence patterns\")\n    print(\"‚Ä¢ Design stakeholder engagement strategies\")\n    \n    return G\n\nprint(\"Design integration functions loaded successfully!\")\nprint(\"\\n‚úÖ All functions for Part 2 are now ready!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 0.3 Design Integration Functions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Algorithm Demonstration Functions\n\ndef demonstrate_kmeans_step_by_step(X=None, n_clusters=3, n_iterations=5):\n    \"\"\"\n    Visualize K-means algorithm step by step.\n    Shows how centers converge to optimal positions.\n    \"\"\"\n    print(\"üéØ K-Means Clustering: Step-by-Step Process\\n\")\n    \n    if X is None:\n        X, y_true = generate_blob_data(n_samples=300, centers=3, cluster_std=0.8)\n    \n    np.random.seed(42)\n    \n    # Initialize random centers\n    idx = np.random.choice(len(X), n_clusters, replace=False)\n    centers = X[idx].copy()\n    \n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    axes = axes.flatten()\n    \n    for iteration in range(n_iterations):\n        ax = axes[iteration]\n        \n        # Assign points to nearest center\n        distances = np.zeros((len(X), n_clusters))\n        for k in range(n_clusters):\n            distances[:, k] = np.linalg.norm(X - centers[k], axis=1)\n        labels = np.argmin(distances, axis=1)\n        \n        # Visualize current state\n        colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n        for k in range(n_clusters):\n            mask = labels == k\n            ax.scatter(X[mask, 0], X[mask, 1], c=colors[k], \n                      s=50, alpha=0.6, label=f'Cluster {k+1}')\n        \n        # Plot centers\n        ax.scatter(centers[:, 0], centers[:, 1], c='red', \n                  marker='*', s=300, edgecolors='black', \n                  linewidth=2, label='Centers', zorder=10)\n        \n        # Update centers\n        new_centers = np.zeros_like(centers)\n        for k in range(n_clusters):\n            if np.sum(labels == k) > 0:\n                new_centers[k] = X[labels == k].mean(axis=0)\n            else:\n                new_centers[k] = centers[k]\n        \n        # Draw movement arrows\n        for k in range(n_clusters):\n            ax.arrow(centers[k, 0], centers[k, 1],\n                    new_centers[k, 0] - centers[k, 0],\n                    new_centers[k, 1] - centers[k, 1],\n                    head_width=0.1, head_length=0.1,\n                    fc='black', ec='black', alpha=0.5)\n        \n        centers = new_centers.copy()\n        \n        ax.set_title(f'Iteration {iteration + 1}', fontsize=12, fontweight='bold')\n        ax.set_xlabel('Feature 1')\n        ax.set_ylabel('Feature 2')\n        if iteration == 0:\n            ax.legend(loc='upper right', fontsize=8)\n    \n    # Final result\n    ax = axes[5]\n    for k in range(n_clusters):\n        mask = labels == k\n        ax.scatter(X[mask, 0], X[mask, 1], c=colors[k], \n                  s=50, alpha=0.6, label=f'Cluster {k+1}')\n    ax.scatter(centers[:, 0], centers[:, 1], c='red', \n              marker='*', s=300, edgecolors='black', \n              linewidth=2, label='Final Centers', zorder=10)\n    ax.set_title('Final Result', fontsize=12, fontweight='bold')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    ax.legend(loc='upper right', fontsize=8)\n    \n    plt.suptitle('K-Means Algorithm: Watch Centers Converge to Optimal Positions', \n                fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    silhouette = silhouette_score(X, labels)\n    print(f\"\\nüìä Algorithm Performance:\")\n    print(f\"Silhouette Score: {silhouette:.3f}\")\n    print(f\"Converged in 5 iterations\")\n    \n    return centers, labels\n\n\ndef demonstrate_kmeans_implementation():\n    \"\"\"\n    Hands-on K-Means implementation with different K values.\n    Shows impact of K on clustering quality.\n    \"\"\"\n    print(\"üîß Hands-on K-Means Implementation\\n\")\n    \n    # Create innovation dataset\n    df, X_scaled, y_true = generate_innovation_data(n_samples=1000, n_features=10, n_clusters=4)\n    \n    print(f\"Dataset shape: {X_scaled.shape}\")\n    print(f\"Features: {', '.join(df.columns[:10])}\\n\")\n    \n    # Apply K-means with different K values\n    k_values = [2, 3, 4, 5, 6]\n    results = {}\n    \n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    axes = axes.flatten()\n    \n    # Use PCA for visualization\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(StandardScaler().fit_transform(X_scaled))\n    \n    for idx, k in enumerate(k_values):\n        # Fit K-means\n        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n        labels = kmeans.fit_predict(X_scaled)\n        \n        # Calculate metrics\n        silhouette = silhouette_score(X_scaled, labels)\n        inertia = kmeans.inertia_\n        \n        results[k] = {\n            'labels': labels,\n            'centers': kmeans.cluster_centers_,\n            'silhouette': silhouette,\n            'inertia': inertia\n        }\n        \n        # Visualize\n        ax = axes[idx]\n        scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, \n                            cmap='viridis', s=20, alpha=0.6)\n        \n        # Plot centers in PCA space\n        centers_pca = pca.transform(StandardScaler().fit_transform(kmeans.cluster_centers_))\n        ax.scatter(centers_pca[:, 0], centers_pca[:, 1],\n                  c='red', marker='*', s=300, \n                  edgecolors='black', linewidth=2)\n        \n        ax.set_title(f'K={k}, Silhouette={silhouette:.3f}', \n                    fontsize=11, fontweight='bold')\n        ax.set_xlabel('First Principal Component')\n        ax.set_ylabel('Second Principal Component')\n        plt.colorbar(scatter, ax=ax)\n    \n    # Hide extra subplot\n    axes[-1].set_visible(False)\n    \n    plt.suptitle('K-Means with Different K Values (PCA Visualization)', \n                fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    # Print comparison\n    print(\"\\nüìä K-Means Results Comparison:\")\n    print(\"K | Silhouette | Inertia\")\n    print(\"-\" * 30)\n    for k, metrics in results.items():\n        print(f\"{k} | {metrics['silhouette']:.3f}      | {metrics['inertia']:.1f}\")\n    \n    # Best K\n    best_k = max(results.keys(), key=lambda k: results[k]['silhouette'])\n    print(f\"\\n‚ú® Best K={best_k} with silhouette score {results[best_k]['silhouette']:.3f}\")\n    \n    return results\n\n\ndef implement_kmeans_from_scratch():\n    \"\"\"\n    Exercise: Implement K-means from scratch.\n    Compare with sklearn implementation.\n    \"\"\"\n    print(\"üéØ Exercise: Implement K-Means from Scratch\\n\")\n    \n    class MyKMeans:\n        \"\"\"Simple K-Means implementation for learning\"\"\"\n        \n        def __init__(self, n_clusters=3, max_iters=100, tol=1e-4):\n            self.n_clusters = n_clusters\n            self.max_iters = max_iters\n            self.tol = tol\n            self.centers = None\n            self.labels = None\n        \n        def fit(self, X):\n            \"\"\"Fit K-means to data\"\"\"\n            n_samples = X.shape[0]\n            \n            # Initialize centers randomly\n            idx = np.random.choice(n_samples, self.n_clusters, replace=False)\n            self.centers = X[idx].copy()\n            \n            for iteration in range(self.max_iters):\n                # Assign points to nearest center\n                distances = np.zeros((n_samples, self.n_clusters))\n                for k in range(self.n_clusters):\n                    distances[:, k] = np.linalg.norm(X - self.centers[k], axis=1)\n                self.labels = np.argmin(distances, axis=1)\n                \n                # Update centers\n                new_centers = np.zeros_like(self.centers)\n                for k in range(self.n_clusters):\n                    if np.sum(self.labels == k) > 0:\n                        new_centers[k] = X[self.labels == k].mean(axis=0)\n                    else:\n                        new_centers[k] = self.centers[k]\n                \n                # Check convergence\n                if np.linalg.norm(new_centers - self.centers) < self.tol:\n                    print(f\"Converged at iteration {iteration + 1}\")\n                    break\n                \n                self.centers = new_centers\n            \n            return self\n        \n        def predict(self, X):\n            \"\"\"Predict cluster labels\"\"\"\n            distances = np.zeros((X.shape[0], self.n_clusters))\n            for k in range(self.n_clusters):\n                distances[:, k] = np.linalg.norm(X - self.centers[k], axis=1)\n            return np.argmin(distances, axis=1)\n    \n    # Test implementation\n    X_test, _ = generate_blob_data(n_samples=200, centers=3)\n    \n    # Your implementation\n    my_kmeans = MyKMeans(n_clusters=3)\n    my_kmeans.fit(X_test)\n    my_labels = my_kmeans.labels\n    \n    # Sklearn implementation\n    sklearn_kmeans = KMeans(n_clusters=3, random_state=42)\n    sklearn_labels = sklearn_kmeans.fit_predict(X_test)\n    \n    # Compare results\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    \n    ax1.scatter(X_test[:, 0], X_test[:, 1], c=my_labels, cmap='viridis', s=50)\n    ax1.scatter(my_kmeans.centers[:, 0], my_kmeans.centers[:, 1],\n               c='red', marker='*', s=300, edgecolors='black', linewidth=2)\n    ax1.set_title('Your Implementation', fontsize=12, fontweight='bold')\n    ax1.set_xlabel('Feature 1')\n    ax1.set_ylabel('Feature 2')\n    \n    ax2.scatter(X_test[:, 0], X_test[:, 1], c=sklearn_labels, cmap='viridis', s=50)\n    ax2.scatter(sklearn_kmeans.cluster_centers_[:, 0], \n               sklearn_kmeans.cluster_centers_[:, 1],\n               c='red', marker='*', s=300, edgecolors='black', linewidth=2)\n    ax2.set_title('Sklearn Implementation', fontsize=12, fontweight='bold')\n    ax2.set_xlabel('Feature 1')\n    ax2.set_ylabel('Feature 2')\n    \n    plt.suptitle('K-Means Implementation Comparison', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\n‚úÖ Your implementation silhouette score: {silhouette_score(X_test, my_labels):.3f}\")\n    print(f\"‚úÖ Sklearn silhouette score: {silhouette_score(X_test, sklearn_labels):.3f}\")\n    \n    return my_kmeans, sklearn_kmeans\n\n\ndef find_optimal_k_elbow():\n    \"\"\"\n    Comprehensive elbow method analysis with multiple metrics.\n    Shows how to find the optimal number of clusters.\n    \"\"\"\n    print(\"üìà Finding Optimal K: The Elbow Method\\n\")\n    \n    # Generate data with known clusters\n    X_elbow, y_true = generate_blob_data(n_samples=500, centers=4, cluster_std=1.0)\n    \n    # Test range of K values\n    k_range = range(1, 11)\n    inertias = []\n    silhouettes = []\n    davies_bouldins = []\n    \n    for k in k_range:\n        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n        labels = kmeans.fit_predict(X_elbow)\n        \n        inertias.append(kmeans.inertia_)\n        \n        if k > 1:  # Metrics need at least 2 clusters\n            silhouettes.append(silhouette_score(X_elbow, labels))\n            davies_bouldins.append(davies_bouldin_score(X_elbow, labels))\n        else:\n            silhouettes.append(0)\n            davies_bouldins.append(0)\n    \n    # Calculate elbow point\n    deltas = np.diff(inertias)\n    delta_deltas = np.diff(deltas)\n    elbow_idx = np.argmax(np.abs(delta_deltas)) + 2  # +2 because of double diff\n    \n    # Visualization\n    fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n    \n    # Inertia/Elbow plot\n    ax1 = axes[0]\n    ax1.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n    ax1.axvline(x=list(k_range)[elbow_idx], color='red', \n               linestyle='--', alpha=0.7, label=f'Elbow at k={list(k_range)[elbow_idx]}')\n    ax1.set_xlabel('Number of Clusters (k)', fontsize=11)\n    ax1.set_ylabel('Inertia', fontsize=11)\n    ax1.set_title('Elbow Method', fontsize=12, fontweight='bold')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # Silhouette scores\n    ax2 = axes[1]\n    ax2.plot(k_range[1:], silhouettes[1:], 'go-', linewidth=2, markersize=8)\n    best_silhouette_k = list(k_range)[np.argmax(silhouettes) if silhouettes else 0]\n    ax2.axvline(x=best_silhouette_k, color='red', linestyle='--', \n               alpha=0.7, label=f'Best at k={best_silhouette_k}')\n    ax2.set_xlabel('Number of Clusters (k)', fontsize=11)\n    ax2.set_ylabel('Silhouette Score', fontsize=11)\n    ax2.set_title('Silhouette Analysis', fontsize=12, fontweight='bold')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    # Davies-Bouldin Index (lower is better)\n    ax3 = axes[2]\n    ax3.plot(k_range[1:], davies_bouldins[1:], 'ro-', linewidth=2, markersize=8)\n    best_db_k = list(k_range)[np.argmin(davies_bouldins[1:]) + 1 if davies_bouldins[1:] else 0]\n    ax3.axvline(x=best_db_k, color='green', linestyle='--', \n               alpha=0.7, label=f'Best at k={best_db_k}')\n    ax3.set_xlabel('Number of Clusters (k)', fontsize=11)\n    ax3.set_ylabel('Davies-Bouldin Index', fontsize=11)\n    ax3.set_title('Davies-Bouldin Index (lower is better)', fontsize=12, fontweight='bold')\n    ax3.legend()\n    ax3.grid(True, alpha=0.3)\n    \n    plt.suptitle('Multiple Methods for Finding Optimal K', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nüìä Optimal K Recommendations:\")\n    print(f\"Elbow Method: k={list(k_range)[elbow_idx]}\")\n    print(f\"Silhouette Score: k={best_silhouette_k}\")\n    print(f\"Davies-Bouldin Index: k={best_db_k}\")\n    print(f\"\\nTrue number of clusters: 4\")\n    print(\"\\nüí° Tip: When methods disagree, consider domain knowledge and use case!\")\n    \n    return {'elbow_k': list(k_range)[elbow_idx], 'silhouette_k': best_silhouette_k, 'db_k': best_db_k}\n\n\ndef demonstrate_dbscan_parameters():\n    \"\"\"\n    DBSCAN parameter exploration showing impact of eps and min_samples.\n    Helps understand how to tune DBSCAN for different datasets.\n    \"\"\"\n    print(\"üîç DBSCAN: Understanding eps and min_samples\\n\")\n    \n    # Generate data with outliers\n    X_dbscan, _ = generate_blob_data(n_samples=300, centers=3, cluster_std=0.5)\n    # Add noise points\n    X_noise = np.random.uniform(-6, 6, (50, 2))\n    X_dbscan = np.vstack([X_dbscan, X_noise])\n    \n    # Test different parameter combinations\n    eps_values = [0.3, 0.5, 0.7, 1.0]\n    min_samples_values = [3, 5, 10, 20]\n    \n    fig, axes = plt.subplots(len(eps_values), len(min_samples_values), \n                            figsize=(16, 12))\n    \n    for i, eps in enumerate(eps_values):\n        for j, min_samples in enumerate(min_samples_values):\n            ax = axes[i, j]\n            \n            # Apply DBSCAN\n            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n            labels = dbscan.fit_predict(X_dbscan)\n            \n            # Count clusters and noise\n            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n            n_noise = list(labels).count(-1)\n            \n            # Plot\n            unique_labels = set(labels)\n            colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n            \n            for k, col in zip(unique_labels, colors):\n                if k == -1:\n                    col = 'black'\n                    marker = 'x'\n                else:\n                    marker = 'o'\n                \n                class_member_mask = (labels == k)\n                xy = X_dbscan[class_member_mask]\n                ax.scatter(xy[:, 0], xy[:, 1], c=[col], \n                          marker=marker, s=30, alpha=0.7)\n            \n            ax.set_title(f'eps={eps}, min={min_samples}\\nC={n_clusters}, N={n_noise}',\n                        fontsize=9)\n            ax.set_xticks([])\n            ax.set_yticks([])\n    \n    # Add labels\n    for i, eps in enumerate(eps_values):\n        axes[i, 0].set_ylabel(f'eps={eps}', fontsize=10, fontweight='bold')\n    for j, min_samples in enumerate(min_samples_values):\n        axes[0, j].set_xlabel(f'min_samples={min_samples}', fontsize=10, fontweight='bold')\n        axes[0, j].xaxis.set_label_position('top')\n    \n    plt.suptitle('DBSCAN Parameter Grid: Impact of eps and min_samples\\n'\n                'C=Clusters, N=Noise points', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nüìö Parameter Guidelines:\")\n    print(\"‚Ä¢ eps: Maximum distance between points in same neighborhood\")\n    print(\"  - Too small: Many clusters, more noise\")\n    print(\"  - Too large: Few clusters, points merge\")\n    print(\"\\n‚Ä¢ min_samples: Minimum points to form dense region\")\n    print(\"  - Too small: More clusters, less noise\")\n    print(\"  - Too large: Fewer clusters, more noise\")\n    print(\"\\nüí° Start with min_samples = 2 * dimensions, adjust eps based on data\")\n    \n    return X_dbscan\n\n\ndef demonstrate_hierarchical_clustering():\n    \"\"\"\n    Hierarchical clustering demonstration with dendrograms.\n    Shows different linkage methods and how to cut the tree.\n    \"\"\"\n    print(\"üå≥ Hierarchical Clustering: Building Innovation Taxonomy\\n\")\n    \n    # Generate hierarchical data\n    X_hier, y_hier = generate_blob_data(n_samples=100, centers=4, cluster_std=0.5)\n    \n    # Different linkage methods\n    linkage_methods = ['ward', 'complete', 'average', 'single']\n    \n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    axes = axes.flatten()\n    \n    for idx, method in enumerate(linkage_methods):\n        ax = axes[idx]\n        \n        # Perform hierarchical clustering\n        from scipy.cluster.hierarchy import dendrogram, linkage\n        linkage_matrix = linkage(X_hier, method=method)\n        \n        # Plot dendrogram\n        dendrogram(linkage_matrix, ax=ax, truncate_mode='level', \n                  p=5, color_threshold=0, above_threshold_color='gray')\n        \n        ax.set_title(f'Linkage: {method.capitalize()}', fontsize=12, fontweight='bold')\n        ax.set_xlabel('Sample Index')\n        ax.set_ylabel('Distance')\n    \n    plt.suptitle('Hierarchical Clustering with Different Linkage Methods', \n                fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nüìä Linkage Method Comparison:\")\n    print(\"‚Ä¢ Ward: Minimizes within-cluster variance (most common)\")\n    print(\"‚Ä¢ Complete: Maximum distance between clusters\")\n    print(\"‚Ä¢ Average: Average distance between all pairs\")\n    print(\"‚Ä¢ Single: Minimum distance (can create chains)\")\n    \n    return X_hier\n\n\ndef demonstrate_gmm():\n    \"\"\"\n    Gaussian Mixture Models demonstration.\n    Shows soft clustering with probabilities.\n    \"\"\"\n    print(\"üîÆ Gaussian Mixture Models: Soft Clustering\\n\")\n    \n    # Generate overlapping clusters\n    X_gmm, y_gmm = generate_blob_data(n_samples=400, centers=3, cluster_std=1.2)\n    \n    # Fit GMM\n    from sklearn.mixture import GaussianMixture\n    gmm = GaussianMixture(n_components=3, random_state=42)\n    gmm.fit(X_gmm)\n    \n    # Get predictions and probabilities\n    gmm_labels = gmm.predict(X_gmm)\n    gmm_probs = gmm.predict_proba(X_gmm)\n    \n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    \n    # Hard clustering\n    ax1 = axes[0]\n    scatter1 = ax1.scatter(X_gmm[:, 0], X_gmm[:, 1], c=gmm_labels, \n                          cmap='viridis', s=30, alpha=0.7)\n    ax1.scatter(gmm.means_[:, 0], gmm.means_[:, 1],\n               c='red', marker='*', s=300, edgecolors='black', linewidth=2)\n    ax1.set_title('Hard Assignment', fontsize=11, fontweight='bold')\n    ax1.set_xlabel('Feature 1')\n    ax1.set_ylabel('Feature 2')\n    plt.colorbar(scatter1, ax=ax1)\n    \n    # Soft clustering - show uncertainty\n    ax2 = axes[1]\n    uncertainty = -np.sum(gmm_probs * np.log(gmm_probs + 1e-10), axis=1)\n    scatter2 = ax2.scatter(X_gmm[:, 0], X_gmm[:, 1], c=uncertainty, \n                          cmap='RdYlGn_r', s=30, alpha=0.7)\n    ax2.set_title('Uncertainty', fontsize=11, fontweight='bold')\n    ax2.set_xlabel('Feature 1')\n    ax2.set_ylabel('Feature 2')\n    plt.colorbar(scatter2, ax=ax2, label='Uncertainty')\n    \n    # Probability contours\n    ax3 = axes[2]\n    x = np.linspace(X_gmm[:, 0].min() - 1, X_gmm[:, 0].max() + 1, 100)\n    y = np.linspace(X_gmm[:, 1].min() - 1, X_gmm[:, 1].max() + 1, 100)\n    X_grid, Y_grid = np.meshgrid(x, y)\n    XX = np.array([X_grid.ravel(), Y_grid.ravel()]).T\n    Z = -gmm.score_samples(XX)\n    Z = Z.reshape(X_grid.shape)\n    \n    ax3.contour(X_grid, Y_grid, Z, levels=10, linewidths=0.5, colors='black', alpha=0.3)\n    ax3.contourf(X_grid, Y_grid, Z, levels=10, cmap='viridis', alpha=0.3)\n    ax3.scatter(X_gmm[:, 0], X_gmm[:, 1], c=gmm_labels, \n               cmap='viridis', s=30, alpha=0.7, edgecolors='black', linewidth=0.5)\n    ax3.scatter(gmm.means_[:, 0], gmm.means_[:, 1],\n               c='red', marker='*', s=300, edgecolors='white', linewidth=2)\n    ax3.set_title('Probability Contours', fontsize=11, fontweight='bold')\n    ax3.set_xlabel('Feature 1')\n    ax3.set_ylabel('Feature 2')\n    \n    plt.suptitle('Gaussian Mixture Models: Soft vs Hard Clustering', \n                fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    # Show probability examples\n    print(\"\\nüìä Example: Innovation Probability Assignments\")\n    print(\"\\nSample 5 innovations and their cluster probabilities:\")\n    print(\"ID | Cluster 1 | Cluster 2 | Cluster 3 | Assigned\")\n    print(\"-\" * 50)\n    for i in range(5):\n        probs = gmm_probs[i]\n        assigned = gmm_labels[i]\n        print(f\"{i:2} | {probs[0]:.3f}    | {probs[1]:.3f}    | \"\n              f\"{probs[2]:.3f}    | Cluster {assigned+1}\")\n    \n    print(\"\\nüí° GMM Benefits:\")\n    print(\"‚Ä¢ Shows uncertainty in cluster assignments\")\n    print(\"‚Ä¢ Handles overlapping clusters\")\n    print(\"‚Ä¢ Provides probability distributions\")\n    \n    return gmm, X_gmm, gmm_labels\n\n\ndef compare_all_algorithms():\n    \"\"\"\n    Comprehensive comparison of all clustering algorithms.\n    Shows strengths and weaknesses of each method.\n    \"\"\"\n    print(\"‚öñÔ∏è Clustering Algorithm Comparison\\n\")\n    \n    # Generate test dataset\n    X_compare, y_compare = generate_blob_data(n_samples=500, centers=4, cluster_std=1.0)\n    \n    # Add some noise\n    X_noise = np.random.uniform(X_compare.min(), X_compare.max(), (50, 2))\n    X_compare = np.vstack([X_compare, X_noise])\n    y_compare = np.hstack([y_compare, [-1] * 50])\n    \n    # Standardize\n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X_compare)\n    \n    # Define algorithms\n    from sklearn.mixture import GaussianMixture\n    algorithms = [\n        ('K-Means', KMeans(n_clusters=4, random_state=42)),\n        ('DBSCAN', DBSCAN(eps=0.3, min_samples=5)),\n        ('Hierarchical', AgglomerativeClustering(n_clusters=4)),\n        ('GMM', GaussianMixture(n_components=4, random_state=42))\n    ]\n    \n    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n    \n    # Store results\n    comparison_results = []\n    \n    for idx, (name, algorithm) in enumerate(algorithms):\n        # Fit algorithm\n        if hasattr(algorithm, 'fit_predict'):\n            labels = algorithm.fit_predict(X_scaled)\n        else:\n            labels = algorithm.fit(X_scaled).predict(X_scaled)\n        \n        # Calculate metrics\n        unique_labels = np.unique(labels[labels != -1])\n        n_clusters = len(unique_labels)\n        n_noise = np.sum(labels == -1)\n        \n        if n_clusters > 1:\n            silhouette = silhouette_score(X_scaled, labels)\n            db_index = davies_bouldin_score(X_scaled, labels)\n        else:\n            silhouette = -1\n            db_index = np.inf\n        \n        comparison_results.append({\n            'Algorithm': name,\n            'Clusters': n_clusters,\n            'Noise': n_noise,\n            'Silhouette': silhouette,\n            'Davies-Bouldin': db_index\n        })\n        \n        # Visualization\n        ax1 = axes[0, idx]\n        ax2 = axes[1, idx]\n        \n        # Plot clusters\n        for label in unique_labels:\n            if label == -1:\n                mask = labels == label\n                ax1.scatter(X_compare[mask, 0], X_compare[mask, 1],\n                           c='black', marker='x', s=30, alpha=0.5, label='Noise')\n            else:\n                mask = labels == label\n                ax1.scatter(X_compare[mask, 0], X_compare[mask, 1],\n                           s=30, alpha=0.7, label=f'C{label}')\n        \n        ax1.set_title(f'{name}\\nClusters: {n_clusters}, Noise: {n_noise}', \n                     fontsize=10, fontweight='bold')\n        ax1.set_xticks([])\n        ax1.set_yticks([])\n        \n        # Metrics bar chart\n        metrics = ['Silhouette', 'DB Index\\n(inverted)']\n        values = [silhouette, -db_index/10]  # Normalize for display\n        colors_bar = ['green' if v > 0 else 'red' for v in values]\n        \n        bars = ax2.bar(range(2), values, color=colors_bar, alpha=0.7)\n        ax2.set_xticks(range(2))\n        ax2.set_xticklabels(metrics, fontsize=8)\n        ax2.set_title(f'{name} Metrics', fontsize=10, fontweight='bold')\n        ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n    \n    plt.suptitle('Clustering Algorithm Comparison', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    # Comparison table\n    comparison_df = pd.DataFrame(comparison_results)\n    print(\"\\nüìä Performance Comparison:\")\n    display(comparison_df)\n    \n    print(\"\\nüéØ Algorithm Selection Guide:\")\n    print(\"\\nüìå K-Means: Fast, simple, spherical clusters\")\n    print(\"üìå DBSCAN: Arbitrary shapes, identifies outliers\")\n    print(\"üìå Hierarchical: Creates taxonomy, no K needed upfront\")\n    print(\"üìå GMM: Soft clustering, overlapping clusters\")\n    \n    return comparison_df\n\n\ndef demonstrate_common_mistakes():\n    \"\"\"\n    Show common clustering mistakes and how to fix them.\n    Educational visualization of pitfalls.\n    \"\"\"\n    print(\"‚ö†Ô∏è Common Clustering Mistakes and How to Fix Them\\n\")\n    \n    # Generate data\n    X_mistakes, y_mistakes = generate_blob_data(n_samples=300, centers=3)\n    \n    fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n    \n    # Mistake 1: Not scaling features\n    ax1 = axes[0, 0]\n    X_unscaled = X_mistakes.copy()\n    X_unscaled[:, 1] *= 100  # Make one feature much larger\n    kmeans_unscaled = KMeans(n_clusters=3, random_state=42)\n    labels_unscaled = kmeans_unscaled.fit_predict(X_unscaled)\n    ax1.scatter(X_unscaled[:, 0], X_unscaled[:, 1], c=labels_unscaled, \n               cmap='viridis', s=30, alpha=0.7)\n    ax1.set_title('‚ùå Mistake: Unscaled Features', fontsize=10, fontweight='bold', color='red')\n    ax1.set_xlabel('Feature 1 (0-10)')\n    ax1.set_ylabel('Feature 2 (0-1000)')\n    \n    # Fix 1: Scale features\n    ax2 = axes[0, 1]\n    scaler = StandardScaler()\n    X_scaled_fix = scaler.fit_transform(X_unscaled)\n    kmeans_scaled = KMeans(n_clusters=3, random_state=42)\n    labels_scaled = kmeans_scaled.fit_predict(X_scaled_fix)\n    ax2.scatter(X_scaled_fix[:, 0], X_scaled_fix[:, 1], c=labels_scaled, \n               cmap='viridis', s=30, alpha=0.7)\n    ax2.set_title('‚úÖ Fix: Scaled Features', fontsize=10, fontweight='bold', color='green')\n    ax2.set_xlabel('Feature 1 (standardized)')\n    ax2.set_ylabel('Feature 2 (standardized)')\n    \n    # Mistake 2: Wrong number of clusters\n    ax3 = axes[0, 2]\n    kmeans_wrong_k = KMeans(n_clusters=10, random_state=42)\n    labels_wrong_k = kmeans_wrong_k.fit_predict(X_mistakes)\n    ax3.scatter(X_mistakes[:, 0], X_mistakes[:, 1], c=labels_wrong_k, \n               cmap='tab10', s=30, alpha=0.7)\n    silhouette_wrong = silhouette_score(X_mistakes, labels_wrong_k)\n    ax3.set_title(f'‚ùå Too Many Clusters (K=10)\\nSilhouette: {silhouette_wrong:.3f}', \n                 fontsize=10, fontweight='bold', color='red')\n    \n    # Fix 2: Use elbow method\n    ax4 = axes[1, 0]\n    kmeans_correct_k = KMeans(n_clusters=3, random_state=42)\n    labels_correct_k = kmeans_correct_k.fit_predict(X_mistakes)\n    ax4.scatter(X_mistakes[:, 0], X_mistakes[:, 1], c=labels_correct_k, \n               cmap='viridis', s=30, alpha=0.7)\n    silhouette_correct = silhouette_score(X_mistakes, labels_correct_k)\n    ax4.set_title(f'‚úÖ Optimal K=3\\nSilhouette: {silhouette_correct:.3f}', \n                 fontsize=10, fontweight='bold', color='green')\n    \n    # Mistake 3: Ignoring outliers\n    ax5 = axes[1, 1]\n    X_with_outliers = X_mistakes.copy()\n    outliers = np.random.uniform(-15, 15, (20, 2))\n    X_with_outliers = np.vstack([X_with_outliers, outliers])\n    kmeans_outliers = KMeans(n_clusters=3, random_state=42)\n    labels_outliers = kmeans_outliers.fit_predict(X_with_outliers)\n    ax5.scatter(X_with_outliers[:, 0], X_with_outliers[:, 1], \n               c=labels_outliers, cmap='viridis', s=30, alpha=0.7)\n    ax5.set_title('‚ùå K-Means with Outliers', fontsize=10, fontweight='bold', color='red')\n    \n    # Fix 3: Use DBSCAN\n    ax6 = axes[1, 2]\n    dbscan_fix = DBSCAN(eps=1.5, min_samples=5)\n    labels_dbscan = dbscan_fix.fit_predict(X_with_outliers)\n    unique_labels = set(labels_dbscan)\n    colors_db = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n    for k, col in zip(unique_labels, colors_db):\n        if k == -1:\n            col = 'black'\n            marker = 'x'\n        else:\n            marker = 'o'\n        class_member_mask = (labels_dbscan == k)\n        xy = X_with_outliers[class_member_mask]\n        ax6.scatter(xy[:, 0], xy[:, 1], c=[col], marker=marker, s=30, alpha=0.7)\n    ax6.set_title('‚úÖ DBSCAN Handles Outliers', fontsize=10, fontweight='bold', color='green')\n    \n    plt.suptitle('Common Clustering Mistakes and Solutions', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nüìö Summary of Common Mistakes:\")\n    print(\"\\n1. üìè Not Scaling Features:\")\n    print(\"   Problem: Features with larger scales dominate\")\n    print(\"   Solution: Always standardize or normalize\")\n    print(\"\\n2. üî¢ Wrong Number of Clusters:\")\n    print(\"   Problem: Too many/few clusters\")\n    print(\"   Solution: Use elbow method, silhouette analysis\")\n    print(\"\\n3. üîç Ignoring Outliers:\")\n    print(\"   Problem: K-means is sensitive to outliers\")\n    print(\"   Solution: Use DBSCAN or remove outliers first\")\n\nprint(\"Algorithm demonstration functions loaded successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 0.2 Algorithm Demonstration Functions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Helper functions for Part 2\n\ndef generate_blob_data(n_samples=1000, centers=3, n_features=2, cluster_std=1.0, random_state=42):\n    \"\"\"Generate simple blob data for demonstrations.\"\"\"\n    from sklearn.datasets import make_blobs\n    X, y = make_blobs(n_samples=n_samples, centers=centers, n_features=n_features,\n                     cluster_std=cluster_std, random_state=random_state)\n    return X, y\n\ndef plot_clusters(X, labels, centers=None, title=\"Clusters\", ax=None):\n    \"\"\"Simple cluster plotting function.\"\"\"\n    if ax is None:\n        import matplotlib.pyplot as plt\n        fig, ax = plt.subplots(figsize=(8, 6))\n    \n    unique_labels = np.unique(labels)\n    colors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))\n    \n    for i, label in enumerate(unique_labels):\n        if label == -1:  # Noise points for DBSCAN\n            mask = labels == label\n            ax.scatter(X[mask, 0], X[mask, 1], c='gray', marker='x', s=50, alpha=0.5, label='Noise')\n        else:\n            mask = labels == label\n            ax.scatter(X[mask, 0], X[mask, 1], c=[colors[i]], s=50, alpha=0.7, label=f'Cluster {label}')\n    \n    if centers is not None:\n        ax.scatter(centers[:, 0], centers[:, 1], c='red', marker='*', s=300, \n                  edgecolors='black', linewidth=2, label='Centers', zorder=10)\n    \n    ax.set_title(title)\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    return ax\n\ndef generate_innovation_data(n_samples=1000, n_features=10, n_clusters=5, noise=0.1):\n    \"\"\"Generate synthetic innovation dataset.\"\"\"\n    from sklearn.datasets import make_blobs\n    X, y = make_blobs(n_samples=n_samples, n_features=n_features, \n                     centers=n_clusters, cluster_std=1.5, random_state=42)\n    X += np.random.normal(0, noise, X.shape)\n    \n    feature_names = [\n        'Tech_Sophistication', 'Market_Readiness', 'Resource_Requirements',\n        'User_Engagement', 'Scalability', 'Innovation_Level',\n        'Competition_Intensity', 'Regulatory_Complexity', 'ROI_Potential',\n        'Implementation_Time'\n    ][:n_features]\n    \n    df = pd.DataFrame(X, columns=feature_names)\n    df['True_Cluster'] = y\n    \n    return df, X, y\n\nprint(\"Helper functions loaded successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 0: All Functions\n\n### 0.1 Helper Functions for Data Generation and Visualization",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3: Part 2 - Technical Deep Dive\n",
    "Master all clustering algorithms with hands-on implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 K-Means Clustering\n",
    "\n",
    "K-Means is the workhorse of clustering algorithms - simple, fast, and effective for many innovation analysis tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means step-by-step visualization\n",
    "centers, labels = demonstrate_kmeans_step_by_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hands-on K-Means implementation\n",
    "results = demonstrate_kmeans_implementation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise: Implement K-Means from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Implement K-means from scratch\n",
    "my_kmeans, sklearn_kmeans = implement_kmeans_from_scratch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Finding Optimal K\n",
    "\n",
    "One of the biggest challenges in clustering: How many clusters should we have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding optimal K with elbow method\n",
    "optimal_k = find_optimal_k_elbow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette analysis for cluster validation\n",
    "demonstrate_silhouette_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 DBSCAN - Density-Based Clustering\n",
    "\n",
    "DBSCAN finds clusters of arbitrary shape and identifies outliers - perfect for innovation data with noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN parameter exploration\n",
    "X_dbscan = demonstrate_dbscan_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm comparison on complex shapes\n",
    "compare_all_algorithms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Hierarchical Clustering\n",
    "\n",
    "Build a tree of clusters - perfect for understanding innovation taxonomies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering demonstration\n",
    "X_hier = demonstrate_hierarchical_clustering()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Gaussian Mixture Models\n",
    "\n",
    "Soft clustering where innovations can belong to multiple categories with different probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Mixture Models demonstration\n",
    "demonstrate_gmm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Algorithm Comparison\n",
    "\n",
    "Let's compare all algorithms on the same dataset to understand their strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive algorithm comparison\n",
    "print(\"‚öñÔ∏è Clustering Algorithm Comparison\\n\")\n",
    "\n",
    "# Generate test dataset\n",
    "X_compare, y_compare = make_blobs(n_samples=500, centers=4, \n",
    "                                 n_features=2, cluster_std=1.0, \n",
    "                                 random_state=42)\n",
    "\n",
    "# Add some noise\n",
    "X_noise = np.random.uniform(X_compare.min(), X_compare.max(), (50, 2))\n",
    "X_compare = np.vstack([X_compare, X_noise])\n",
    "y_compare = np.hstack([y_compare, [-1] * 50])\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_compare)\n",
    "\n",
    "# Define algorithms\n",
    "algorithms = [\n",
    "    ('K-Means', KMeans(n_clusters=4, random_state=42)),\n",
    "    ('DBSCAN', DBSCAN(eps=0.3, min_samples=5)),\n",
    "    ('Hierarchical', AgglomerativeClustering(n_clusters=4)),\n",
    "    ('GMM', GaussianMixture(n_components=4, random_state=42))\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# Store results\n",
    "comparison_results = []\n",
    "\n",
    "for idx, (name, algorithm) in enumerate(algorithms):\n",
    "    # Fit algorithm\n",
    "    if hasattr(algorithm, 'fit_predict'):\n",
    "        labels = algorithm.fit_predict(X_scaled)\n",
    "    else:\n",
    "        labels = algorithm.fit(X_scaled).predict(X_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    unique_labels = np.unique(labels[labels != -1])\n",
    "    n_clusters = len(unique_labels)\n",
    "    n_noise = np.sum(labels == -1)\n",
    "    \n",
    "    if n_clusters > 1:\n",
    "        silhouette = silhouette_score(X_scaled, labels)\n",
    "        db_index = davies_bouldin_score(X_scaled, labels)\n",
    "        ch_index = calinski_harabasz_score(X_scaled, labels)\n",
    "    else:\n",
    "        silhouette = -1\n",
    "        db_index = np.inf\n",
    "        ch_index = 0\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'Algorithm': name,\n",
    "        'Clusters': n_clusters,\n",
    "        'Noise': n_noise,\n",
    "        'Silhouette': silhouette,\n",
    "        'Davies-Bouldin': db_index,\n",
    "        'Calinski-Harabasz': ch_index\n",
    "    })\n",
    "    \n",
    "    # Visualization\n",
    "    ax1 = axes[0, idx]\n",
    "    ax2 = axes[1, idx]\n",
    "    \n",
    "    # Plot clusters\n",
    "    for label in unique_labels:\n",
    "        if label == -1:\n",
    "            mask = labels == label\n",
    "            ax1.scatter(X_compare[mask, 0], X_compare[mask, 1],\n",
    "                       c='black', marker='x', s=30, alpha=0.5, label='Noise')\n",
    "        else:\n",
    "            mask = labels == label\n",
    "            ax1.scatter(X_compare[mask, 0], X_compare[mask, 1],\n",
    "                       s=30, alpha=0.7, label=f'C{label}')\n",
    "    \n",
    "    # Add centers if available\n",
    "    if hasattr(algorithm, 'cluster_centers_'):\n",
    "        centers = scaler.inverse_transform(algorithm.cluster_centers_)\n",
    "        ax1.scatter(centers[:, 0], centers[:, 1],\n",
    "                   c='red', marker='*', s=200, \n",
    "                   edgecolors='black', linewidth=1.5)\n",
    "    elif hasattr(algorithm, 'means_'):\n",
    "        centers = scaler.inverse_transform(algorithm.means_)\n",
    "        ax1.scatter(centers[:, 0], centers[:, 1],\n",
    "                   c='red', marker='*', s=200, \n",
    "                   edgecolors='black', linewidth=1.5)\n",
    "    \n",
    "    ax1.set_title(f'{name}\\nClusters: {n_clusters}, Noise: {n_noise}', \n",
    "                 fontsize=10, fontweight='bold')\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "    \n",
    "    # Metrics bar chart\n",
    "    metrics = ['Silhouette', 'Davies-Bouldin\\n(lower better)', \n",
    "              'Calinski-Harabasz\\n(higher better)']\n",
    "    values = [silhouette, -db_index/10, ch_index/1000]  # Normalize for display\n",
    "    colors_bar = ['green' if v > 0 else 'red' for v in values]\n",
    "    \n",
    "    bars = ax2.bar(range(3), values, color=colors_bar, alpha=0.7)\n",
    "    ax2.set_xticks(range(3))\n",
    "    ax2.set_xticklabels(metrics, fontsize=8)\n",
    "    ax2.set_title(f'{name} Metrics', fontsize=10, fontweight='bold')\n",
    "    ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val, orig in zip(bars, values, [silhouette, db_index, ch_index]):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{orig:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.suptitle('Clustering Algorithm Comparison: Same Data, Different Approaches', \n",
    "            fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comparison table\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(\"\\nüìä Performance Comparison:\")\n",
    "display(comparison_df)\n",
    "\n",
    "print(\"\\nüéØ Algorithm Selection Guide:\")\n",
    "print(\"\\nüìå K-Means:\")\n",
    "print(\"  ‚úÖ Fast, simple, well-understood\")\n",
    "print(\"  ‚úÖ Good for spherical clusters\")\n",
    "print(\"  ‚ùå Requires K specification\")\n",
    "print(\"  ‚ùå Sensitive to outliers\")\n",
    "\n",
    "print(\"\\nüìå DBSCAN:\")\n",
    "print(\"  ‚úÖ Finds arbitrary shapes\")\n",
    "print(\"  ‚úÖ Identifies outliers\")\n",
    "print(\"  ‚ùå Sensitive to parameters\")\n",
    "print(\"  ‚ùå Struggles with varying densities\")\n",
    "\n",
    "print(\"\\nüìå Hierarchical:\")\n",
    "print(\"  ‚úÖ No need to specify K upfront\")\n",
    "print(\"  ‚úÖ Creates taxonomy\")\n",
    "print(\"  ‚ùå Computationally expensive\")\n",
    "print(\"  ‚ùå Hard to interpret large dendrograms\")\n",
    "\n",
    "print(\"\\nüìå GMM:\")\n",
    "print(\"  ‚úÖ Soft clustering\")\n",
    "print(\"  ‚úÖ Handles overlapping clusters\")\n",
    "print(\"  ‚ùå Assumes Gaussian distribution\")\n",
    "print(\"  ‚ùå Can overfit with many components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Mistakes Gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common clustering mistakes and solutions\n",
    "demonstrate_common_mistakes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 4: Part 3 - Design Integration\n",
    "Transform technical clustering results into actionable innovation insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 From Data Points to Innovation Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform clusters into innovation insights\n",
    "transform_clusters_to_insights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Creating Innovation Archetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create innovation archetypes from clusters\n",
    "create_innovation_archetypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Innovation Taxonomy & Lifecycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build innovation taxonomy\n",
    "build_innovation_taxonomy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Opportunity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate innovation opportunity analysis\n",
    "generate_opportunity_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Innovation Ecosystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create innovation ecosystem visualization\n",
    "create_innovation_ecosystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Part 2 Summary\n",
    "\n",
    "### Technical Skills Mastered:\n",
    "1. **K-Means**: Understanding and implementing from scratch\n",
    "2. **Optimal K**: Multiple methods for finding best clusters\n",
    "3. **DBSCAN**: Handling complex shapes and outliers\n",
    "4. **Hierarchical**: Building taxonomies and dendrograms\n",
    "5. **GMM**: Soft clustering with probabilities\n",
    "6. **Comparison**: Choosing the right algorithm\n",
    "\n",
    "### Design Applications Learned:\n",
    "1. **Innovation Archetypes**: Data-driven personas\n",
    "2. **Opportunity Heatmaps**: Identifying white spaces\n",
    "3. **Priority Matrices**: Strategic resource allocation\n",
    "4. **Ecosystem Networks**: Understanding connections\n",
    "5. **Innovation Taxonomy**: Hierarchical organization\n",
    "\n",
    "### Next: Part 3 - Practice & Advanced Topics\n",
    "Apply everything with real case studies and advanced visualizations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Part 2 Complete: Technical & Design Integration\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nYou've completed:\")\n",
    "print(\"‚Ä¢ Section 3: All clustering algorithms\")\n",
    "print(\"‚Ä¢ Section 4: Design integration and applications\")\n",
    "print(\"\\nüìö Ready for Part 3: Practice, Case Studies, and Advanced Topics\")\n",
    "print(\"\\nContinue with Week01_Part3_Practice_Advanced.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}